{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Rice",
   "id": "85e62e9b0d614c28"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Standard KNN",
   "id": "9a95e66b64a9e508"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T08:12:17.792542Z",
     "start_time": "2025-05-18T08:12:17.789494Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from scipy.io import arff\n",
    "import logging\n",
    "from scipy import stats\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "                    handlers=[logging.FileHandler('k1_rice_standard_knn.log'), logging.StreamHandler()])\n",
    "\n",
    "data_path = 'rice/Rice_Cammeo_Osmancik.arff'\n",
    "data, meta = arff.loadarff(data_path)\n",
    "data_df = pd.DataFrame(data)\n",
    "\n",
    "data_df['Class'] = data_df['Class'].apply(lambda x: x.decode('utf-8'))\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "data_df['Class'] = label_encoder.fit_transform(data_df['Class'])\n",
    "\n",
    "X = data_df.drop(columns=['Class'])\n",
    "y = data_df['Class']\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "k_value = 1\n",
    "\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "times = []\n",
    "best_y_test = None\n",
    "best_predictions = None\n",
    "\n",
    "def evaluate_robustness(best_model, X_test, y_test, noise_levels=[0.1, 0.2, 0.3, 0.4, 0.5], n_bootstrap=100):\n",
    "    results = []\n",
    "\n",
    "    n_samples = len(y_test)\n",
    "\n",
    "    original_pred = best_model.predict(X_test)\n",
    "    original_acc = accuracy_score(y_test, original_pred)\n",
    "    original_f1 = f1_score(y_test, original_pred, average='weighted')\n",
    "    results.append(('No Noise', original_acc, original_f1))\n",
    "\n",
    "    bootstrap_original_acc = []\n",
    "    bootstrap_original_f1 = []\n",
    "\n",
    "    for _ in range(n_bootstrap):\n",
    "        indices = np.random.choice(n_samples, n_samples, replace=True)\n",
    "        X_boot = X_test[indices]\n",
    "        y_boot = y_test.iloc[indices] if hasattr(y_test, 'iloc') else y_test[indices]\n",
    "\n",
    "        boot_pred = best_model.predict(X_boot)\n",
    "        bootstrap_original_acc.append(accuracy_score(y_boot, boot_pred))\n",
    "        bootstrap_original_f1.append(f1_score(y_boot, boot_pred, average='weighted'))\n",
    "\n",
    "    p_values_acc = []\n",
    "    p_values_f1 = []\n",
    "    bootstrap_acc_all = []\n",
    "    bootstrap_f1_all = []\n",
    "\n",
    "    for noise in noise_levels:\n",
    "        bootstrap_acc = []\n",
    "        bootstrap_f1 = []\n",
    "\n",
    "        for _ in range(n_bootstrap):\n",
    "            indices = np.random.choice(n_samples, n_samples, replace=True)\n",
    "            X_boot = X_test[indices].copy()\n",
    "            y_boot = y_test.iloc[indices] if hasattr(y_test, 'iloc') else y_test[indices]\n",
    "\n",
    "            noise_matrix = np.random.normal(0, noise, X_boot.shape)\n",
    "            X_boot += noise_matrix\n",
    "\n",
    "            noisy_pred = best_model.predict(X_boot)\n",
    "            noisy_acc = accuracy_score(y_boot, noisy_pred)\n",
    "            noisy_f1 = f1_score(y_boot, noisy_pred, average='weighted')\n",
    "\n",
    "            bootstrap_acc.append(noisy_acc)\n",
    "            bootstrap_f1.append(noisy_f1)\n",
    "\n",
    "        mean_acc = np.mean(bootstrap_acc)\n",
    "        mean_f1 = np.mean(bootstrap_f1)\n",
    "\n",
    "        t_stat_acc, p_val_acc = stats.ttest_rel(bootstrap_original_acc, bootstrap_acc)\n",
    "        t_stat_f1, p_val_f1 = stats.ttest_rel(bootstrap_original_f1, bootstrap_f1)\n",
    "\n",
    "        results.append((f'Noise {noise:.2f}', mean_acc, mean_f1))\n",
    "        p_values_acc.append(p_val_acc)\n",
    "        p_values_f1.append(p_val_f1)\n",
    "        bootstrap_acc_all.append(bootstrap_acc)\n",
    "        bootstrap_f1_all.append(bootstrap_f1)\n",
    "\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    noise_labels = [r[0] for r in results]\n",
    "    acc_values = [r[1] for r in results]\n",
    "    f1_values = [r[2] for r in results]\n",
    "\n",
    "    x = np.arange(len(noise_labels))\n",
    "    width = 0.35\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    bars_acc = plt.bar(x - width/2, acc_values, width, label='Accuracy')\n",
    "    bars_f1 = plt.bar(x + width/2, f1_values, width, label='F1 Score')\n",
    "    plt.xlabel('Noise Level')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Model Robustness to Noise')\n",
    "    plt.xticks(x, noise_labels, rotation=45)\n",
    "    plt.legend()\n",
    "\n",
    "    for i in range(len(p_values_acc)):\n",
    "        if i > 0:\n",
    "            if p_values_acc[i-1] < 0.001:\n",
    "                plt.text(i - width/2, acc_values[i] - 0.02, '***', ha='center')\n",
    "            elif p_values_acc[i-1] < 0.01:\n",
    "                plt.text(i - width/2, acc_values[i] - 0.02, '**', ha='center')\n",
    "            elif p_values_acc[i-1] < 0.05:\n",
    "                plt.text(i - width/2, acc_values[i] - 0.02, '*', ha='center')\n",
    "\n",
    "            if p_values_f1[i-1] < 0.001:\n",
    "                plt.text(i + width/2, f1_values[i] - 0.02, '***', ha='center')\n",
    "            elif p_values_f1[i-1] < 0.01:\n",
    "                plt.text(i + width/2, f1_values[i] - 0.02, '**', ha='center')\n",
    "            elif p_values_f1[i-1] < 0.05:\n",
    "                plt.text(i + width/2, f1_values[i] - 0.02, '*', ha='center')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    bars_acc = plt.bar(x - width/2, acc_values, width, label='Accuracy')\n",
    "    bars_f1 = plt.bar(x + width/2, f1_values, width, label='F1 Score')\n",
    "    plt.xlabel('Noise Level')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Zoomed View (0.85-1.0 range) with p-values')\n",
    "    plt.xticks(x, noise_labels, rotation=45)\n",
    "    plt.ylim(0.85, 1.0)\n",
    "\n",
    "    for i in range(len(p_values_acc)):\n",
    "        if i > 0:\n",
    "            if p_values_acc[i-1] < 0.001:\n",
    "                plt.text(i - width/2, acc_values[i] + 0.005, '***', ha='center', fontsize=10)\n",
    "            elif p_values_acc[i-1] < 0.01:\n",
    "                plt.text(i - width/2, acc_values[i] + 0.005, '**', ha='center', fontsize=10)\n",
    "            elif p_values_acc[i-1] < 0.05:\n",
    "                plt.text(i - width/2, acc_values[i] + 0.005, '*', ha='center', fontsize=10)\n",
    "\n",
    "            if p_values_f1[i-1] < 0.001:\n",
    "                plt.text(i + width/2, f1_values[i] + 0.005, '***', ha='center', fontsize=10)\n",
    "            elif p_values_f1[i-1] < 0.01:\n",
    "                plt.text(i + width/2, f1_values[i] + 0.005, '**', ha='center', fontsize=10)\n",
    "            elif p_values_f1[i-1] < 0.05:\n",
    "                plt.text(i + width/2, f1_values[i] + 0.005, '*', ha='center', fontsize=10)\n",
    "\n",
    "    for i, (acc, f1) in enumerate(zip(acc_values, f1_values)):\n",
    "        plt.text(i - width/2, acc - 0.01, f'{acc:.7f}', ha='center', va='bottom', fontsize=8, rotation=90)\n",
    "        plt.text(i + width/2, f1 - 0.01, f'{f1:.7f}', ha='center', va='bottom', fontsize=8, rotation=90)\n",
    "\n",
    "    plt.figtext(0.5, 0.01, \"* p<0.05, ** p<0.01, *** p<0.001\", ha=\"center\", fontsize=10)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'standard_knn_k{k_value}_noise_robustness.png')\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(14, 8))\n",
    "\n",
    "    plt.subplot(2, 1, 1)\n",
    "    boxplot_data_acc = [bootstrap_original_acc] + bootstrap_acc_all\n",
    "    plt.boxplot(boxplot_data_acc, labels=noise_labels, showfliers=False)\n",
    "    plt.title('Distribution of Accuracy Scores Across Noise Levels')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    boxplot_data_f1 = [bootstrap_original_f1] + bootstrap_f1_all\n",
    "    plt.boxplot(boxplot_data_f1, labels=noise_labels, showfliers=False)\n",
    "    plt.title('Distribution of F1 Scores Across Noise Levels')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'standard_knn_k{k_value}_noise_distribution.png')\n",
    "    plt.close()\n",
    "\n",
    "    logging.info(\"\\n===== NOISE ROBUSTNESS RESULTS WITH P-VALUES =====\")\n",
    "    logging.info(f\"{'Noise Level':<12} {'Accuracy':<12} {'F1 Score':<12} {'p-val (Acc)':<12} {'p-val (F1)':<12}\")\n",
    "\n",
    "    logging.info(f\"{noise_labels[0]:<12} {acc_values[0]:.7f}      {f1_values[0]:.7f}      -            -\")\n",
    "\n",
    "    for i in range(1, len(noise_labels)):\n",
    "        logging.info(f\"{noise_labels[i]:<12} {acc_values[i]:.7f}      {f1_values[i]:.7f}      {p_values_acc[i-1]:.2e}     {p_values_f1[i-1]:.2e}\")\n",
    "\n",
    "    return results, p_values_acc, p_values_f1\n",
    "\n",
    "def analyze_results():\n",
    "\n",
    "    acc_mean, acc_std = np.mean(accuracies), np.std(accuracies)\n",
    "    prec_mean, prec_std = np.mean(precisions), np.std(precisions)\n",
    "    rec_mean, rec_std = np.mean(recalls), np.std(recalls)\n",
    "    f1_mean, f1_std = np.mean(f1_scores), np.std(f1_scores)\n",
    "    time_mean, time_std = np.mean(times), np.std(times)\n",
    "\n",
    "    logging.info(\"\\n===== RESULTS (Standard KNN) =====\")\n",
    "    logging.info(f\"k value (n_neighbors): {k_value}\")\n",
    "    logging.info(f\"Accuracy: {acc_mean:.7f} ± {acc_std:.7f}\")\n",
    "    logging.info(f\"Precision: {prec_mean:.7f} ± {prec_std:.7f}\")\n",
    "    logging.info(f\"Recall: {rec_mean:.7f} ± {rec_std:.7f}\")\n",
    "    logging.info(f\"F1 Score: {f1_mean:.7f} ± {f1_std:.7f}\")\n",
    "    logging.info(f\"Average execution time: {time_mean:.7f}s ± {time_std:.7f}s\")\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "    values = [acc_mean, prec_mean, rec_mean, f1_mean]\n",
    "    errors = [acc_std, prec_std, rec_std, f1_std]\n",
    "\n",
    "    bars = plt.bar(metrics, values, yerr=errors, capsize=10)\n",
    "    plt.title(f'Performance Metrics with Error Bars (Standard KNN, n_neighbors={k_value})')\n",
    "    plt.ylabel('Score')\n",
    "    plt.ylim(0, 1.1)\n",
    "\n",
    "    for bar, val, err in zip(bars, values, errors):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., val + err + 0.02,\n",
    "                f'{val:.7f}±{err:.7f}', ha='center', va='bottom', rotation=0)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'standard_knn_k{k_value}_performance_metrics.png')\n",
    "    plt.close()\n",
    "\n",
    "    if best_y_test is not None and best_predictions is not None:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        cm = confusion_matrix(best_y_test, best_predictions)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.title(f'Confusion Matrix for Standard KNN (k={k_value})')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Actual')\n",
    "        plt.savefig(f'standard_knn_k{k_value}_confusion_matrix.png')\n",
    "        plt.close()\n",
    "\n",
    "def main():\n",
    "    global accuracies, precisions, recalls, f1_scores, times, best_y_test, best_predictions\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    logging.info(f\"Standard KNN test started (k={k_value})...\")\n",
    "\n",
    "    best_acc = -float('inf')\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        fold_start_time = time.time()\n",
    "\n",
    "        knn = KNeighborsClassifier(n_neighbors=k_value)\n",
    "        knn.fit(X_train_scaled, y_train)\n",
    "\n",
    "        predictions = knn.predict(X_test_scaled)\n",
    "\n",
    "        fold_end_time = time.time()\n",
    "        exec_time = fold_end_time - fold_start_time\n",
    "\n",
    "        accuracy = accuracy_score(y_test, predictions)\n",
    "        precision = precision_score(y_test, predictions, average='weighted')\n",
    "        recall = recall_score(y_test, predictions, average='weighted')\n",
    "        f1 = f1_score(y_test, predictions, average='weighted')\n",
    "\n",
    "        accuracies.append(accuracy)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1_scores.append(f1)\n",
    "        times.append(exec_time)\n",
    "\n",
    "        logging.info(f\"Fold, Acc: {accuracy:.7f}, Prec: {precision:.7f}, Rec: {recall:.7f}, F1: {f1:.7f}, Time: {exec_time:.7f}s\")\n",
    "\n",
    "        if accuracy > best_acc:\n",
    "            best_acc = accuracy\n",
    "            best_y_test = y_test\n",
    "            best_predictions = predictions\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    logging.info(f\"Test completed. Total time: {total_time:.2f} second\")\n",
    "\n",
    "    analyze_results()\n",
    "\n",
    "    logging.info(\"Noise resistance test started...\")\n",
    "\n",
    "    best_fold_idx = np.argmax(accuracies)\n",
    "    train_indices = list(kf.split(X))[best_fold_idx][0]\n",
    "    test_indices = list(kf.split(X))[best_fold_idx][1]\n",
    "\n",
    "    X_train = X.iloc[train_indices]\n",
    "    X_test = X.iloc[test_indices]\n",
    "    y_train = y[train_indices]\n",
    "    y_test = y[test_indices]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    best_model = KNeighborsClassifier(n_neighbors=k_value)\n",
    "    best_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    noise_levels = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "    n_bootstrap = 100\n",
    "\n",
    "    noise_results, p_values_acc, p_values_f1 = evaluate_robustness(\n",
    "        best_model, X_test_scaled, y_test, noise_levels, n_bootstrap)\n",
    "\n",
    "    logging.info(\"Code snippet completed...\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "a5b9b59552db813c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 10% Train-data KNN",
   "id": "3c59cbd48b62f666"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T08:12:16.217033Z",
     "start_time": "2025-05-18T08:12:16.214311Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from scipy.io import arff\n",
    "import logging\n",
    "from scipy import stats\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "                    handlers=[logging.FileHandler('k1_rice_one_tenth_train_data_knn.log'), logging.StreamHandler()])\n",
    "\n",
    "data_path = 'rice/Rice_Cammeo_Osmancik.arff'\n",
    "data, meta = arff.loadarff(data_path)\n",
    "data_df = pd.DataFrame(data)\n",
    "\n",
    "data_df['Class'] = data_df['Class'].apply(lambda x: x.decode('utf-8'))\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "data_df['Class'] = label_encoder.fit_transform(data_df['Class'])\n",
    "logging.info(f\"Class Labels: {dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))}\")\n",
    "\n",
    "\n",
    "X = data_df.drop(columns=['Class'])\n",
    "y = data_df['Class']\n",
    "\n",
    "N_OUTER_SPLITS = 5\n",
    "kf = KFold(n_splits=N_OUTER_SPLITS, shuffle=True, random_state=42)\n",
    "\n",
    "k_value = 1\n",
    "N_INNER_SPLITS = 10\n",
    "\n",
    "all_individual_accuracies = []\n",
    "all_individual_precisions = []\n",
    "all_individual_recalls = []\n",
    "all_individual_f1_scores = []\n",
    "all_individual_times = []\n",
    "\n",
    "all_inner_model_details = []\n",
    "\n",
    "\n",
    "def evaluate_robustness(model_to_test, X_test, y_test, noise_levels=[0.1, 0.2, 0.3, 0.4, 0.5], n_bootstrap=100, model_desc=\"Test Model\"):\n",
    "    results = []\n",
    "    logging.info(f\"evaluate_robustness for {model_desc} - X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "\n",
    "    n_samples = len(y_test)\n",
    "    if n_samples == 0:\n",
    "        logging.warning(f\"evaluate_robustness ({model_desc}): Test data is empty, bootstrap cannot be done.\")\n",
    "        for noise in noise_levels: results.append((f'Noise {noise:.2f}', np.nan, np.nan))\n",
    "        return results, [np.nan]*len(noise_levels), [np.nan]*len(noise_levels)\n",
    "\n",
    "    original_pred = model_to_test.predict(X_test)\n",
    "    original_acc = accuracy_score(y_test, original_pred)\n",
    "    original_f1 = f1_score(y_test, original_pred, average='weighted', zero_division=0)\n",
    "    results.append(('No Noise', original_acc, original_f1))\n",
    "    logging.info(f\"evaluate_robustness ({model_desc}) - No Noise: Acc={original_acc:.7f}, F1={original_f1:.7f}\")\n",
    "\n",
    "    bootstrap_original_acc = []\n",
    "    bootstrap_original_f1 = []\n",
    "    for _ in range(n_bootstrap):\n",
    "        indices = np.random.choice(n_samples, n_samples, replace=True)\n",
    "        X_boot, y_boot = X_test[indices], (y_test.iloc[indices] if hasattr(y_test, 'iloc') else y_test[indices])\n",
    "        boot_pred = model_to_test.predict(X_boot)\n",
    "        bootstrap_original_acc.append(accuracy_score(y_boot, boot_pred))\n",
    "        bootstrap_original_f1.append(f1_score(y_boot, boot_pred, average='weighted', zero_division=0))\n",
    "\n",
    "    p_values_acc, p_values_f1, bootstrap_acc_all, bootstrap_f1_all = [], [], [], []\n",
    "\n",
    "    for noise in noise_levels:\n",
    "        bootstrap_acc, bootstrap_f1 = [], []\n",
    "        for _ in range(n_bootstrap):\n",
    "            indices = np.random.choice(n_samples, n_samples, replace=True)\n",
    "            X_boot, y_boot = X_test[indices].copy(), (y_test.iloc[indices] if hasattr(y_test, 'iloc') else y_test[indices])\n",
    "            noise_matrix = np.random.normal(0, noise, X_boot.shape)\n",
    "            X_boot += noise_matrix\n",
    "            noisy_pred = model_to_test.predict(X_boot)\n",
    "            bootstrap_acc.append(accuracy_score(y_boot, noisy_pred))\n",
    "            bootstrap_f1.append(f1_score(y_boot, noisy_pred, average='weighted', zero_division=0))\n",
    "\n",
    "        mean_acc, mean_f1 = (np.mean(bootstrap_acc) if bootstrap_acc else np.nan), (np.mean(bootstrap_f1) if bootstrap_f1 else np.nan)\n",
    "        p_val_acc, p_val_f1 = (stats.ttest_rel(bootstrap_original_acc, bootstrap_acc)[1] if bootstrap_original_acc and bootstrap_acc else np.nan), \\\n",
    "                              (stats.ttest_rel(bootstrap_original_f1, bootstrap_f1)[1] if bootstrap_original_f1 and bootstrap_f1 else np.nan)\n",
    "\n",
    "        results.append((f'Noise {noise:.2f}', mean_acc, mean_f1))\n",
    "        p_values_acc.append(p_val_acc); p_values_f1.append(p_val_f1)\n",
    "        bootstrap_acc_all.append(bootstrap_acc); bootstrap_f1_all.append(bootstrap_f1)\n",
    "\n",
    "    plt.figure(figsize=(13, 7))\n",
    "    noise_labels = [r[0] for r in results]\n",
    "    acc_values = [r[1] for r in results]\n",
    "    f1_values_plot = [r[2] for r in results]\n",
    "    x_indices = np.arange(len(noise_labels))\n",
    "    width = 0.35\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.bar(x_indices - width/2, acc_values, width, label='Accuracy')\n",
    "    plt.bar(x_indices + width/2, f1_values_plot, width, label='F1 Score')\n",
    "    plt.xlabel('Noise Level'); plt.ylabel('Score'); plt.title(f'Model Robustness to Noise ({model_desc})')\n",
    "    plt.xticks(x_indices, noise_labels, rotation=45); plt.legend()\n",
    "    for i in range(len(p_values_acc)):\n",
    "        y_star_acc = acc_values[i+1] - 0.02 if not np.isnan(acc_values[i+1]) else 0\n",
    "        y_star_f1 = f1_values_plot[i+1] - 0.02 if not np.isnan(f1_values_plot[i+1]) else 0\n",
    "        if not np.isnan(p_values_acc[i]):\n",
    "            if p_values_acc[i] < 0.001: plt.text(x_indices[i+1] - width/2, y_star_acc, '***', ha='center')\n",
    "            elif p_values_acc[i] < 0.01: plt.text(x_indices[i+1] - width/2, y_star_acc, '**', ha='center')\n",
    "            elif p_values_acc[i] < 0.05: plt.text(x_indices[i+1] - width/2, y_star_acc, '*', ha='center')\n",
    "        if not np.isnan(p_values_f1[i]):\n",
    "            if p_values_f1[i] < 0.001: plt.text(x_indices[i+1] + width/2, y_star_f1, '***', ha='center')\n",
    "            elif p_values_f1[i] < 0.01: plt.text(x_indices[i+1] + width/2, y_star_f1, '**', ha='center')\n",
    "            elif p_values_f1[i] < 0.05: plt.text(x_indices[i+1] + width/2, y_star_f1, '*', ha='center')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    valid_scores = [s for s in acc_values + f1_values_plot if not np.isnan(s)]\n",
    "    zoom_min_val = max(0.0, min(valid_scores) - 0.05) if valid_scores else 0.0\n",
    "    zoom_max_val = min(1.0, max(valid_scores) + 0.05) if valid_scores else 1.0\n",
    "    if zoom_min_val >= zoom_max_val: zoom_min_val, zoom_max_val = 0.0, 1.0\n",
    "\n",
    "    plt.bar(x_indices - width/2, acc_values, width, label='Accuracy')\n",
    "    plt.bar(x_indices + width/2, f1_values_plot, width, label='F1 Score')\n",
    "    plt.xlabel('Noise Level'); plt.ylabel('Score'); plt.title(f'Zoomed View ({zoom_min_val:.2f}-{zoom_max_val:.2f})')\n",
    "    plt.xticks(x_indices, noise_labels, rotation=45); plt.ylim(zoom_min_val, zoom_max_val)\n",
    "    for i in range(len(p_values_acc)):\n",
    "        if not np.isnan(acc_values[i+1]):\n",
    "            y_text_acc = acc_values[i+1] + 0.005 if acc_values[i+1] < zoom_max_val - 0.01 else acc_values[i+1] - 0.01\n",
    "            plt.text(x_indices[i+1] - width/2, acc_values[i+1] - (zoom_max_val-zoom_min_val)*0.05 if acc_values[i+1] > zoom_min_val + (zoom_max_val-zoom_min_val)*0.1 else acc_values[i+1] + (zoom_max_val-zoom_min_val)*0.02, f'{acc_values[i+1]:.3f}', ha='center', va='bottom' if acc_values[i+1] > zoom_min_val + (zoom_max_val-zoom_min_val)*0.1 else 'top', fontsize=7, rotation=90)\n",
    "            if not np.isnan(p_values_acc[i]):\n",
    "                if p_values_acc[i] < 0.001: plt.text(x_indices[i+1] - width/2, y_text_acc, '***', ha='center', fontsize=10)\n",
    "                elif p_values_acc[i] < 0.01: plt.text(x_indices[i+1] - width/2, y_text_acc, '**', ha='center', fontsize=10)\n",
    "                elif p_values_acc[i] < 0.05: plt.text(x_indices[i+1] - width/2, y_text_acc, '*', ha='center', fontsize=10)\n",
    "        if not np.isnan(f1_values_plot[i+1]):\n",
    "            y_text_f1 = f1_values_plot[i+1] + 0.005 if f1_values_plot[i+1] < zoom_max_val - 0.01 else f1_values_plot[i+1] - 0.01\n",
    "            plt.text(x_indices[i+1] + width/2, f1_values_plot[i+1] - (zoom_max_val-zoom_min_val)*0.05 if f1_values_plot[i+1] > zoom_min_val + (zoom_max_val-zoom_min_val)*0.1 else f1_values_plot[i+1] + (zoom_max_val-zoom_min_val)*0.02, f'{f1_values_plot[i+1]:.3f}', ha='center', va='bottom' if f1_values_plot[i+1] > zoom_min_val + (zoom_max_val-zoom_min_val)*0.1 else 'top', fontsize=7, rotation=90)\n",
    "            if not np.isnan(p_values_f1[i]):\n",
    "                if p_values_f1[i] < 0.001: plt.text(x_indices[i+1] + width/2, y_text_f1, '***', ha='center', fontsize=10)\n",
    "                elif p_values_f1[i] < 0.01: plt.text(x_indices[i+1] + width/2, y_text_f1, '**', ha='center', fontsize=10)\n",
    "                elif p_values_f1[i] < 0.05: plt.text(x_indices[i+1] + width/2, y_text_f1, '*', ha='center', fontsize=10)\n",
    "\n",
    "    plt.figtext(0.5, 0.01, \"* p<0.05, ** p<0.01, *** p<0.001 (vs No Noise)\", ha=\"center\", fontsize=10)\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.suptitle(f'Model Robustness ({model_desc}, k={k_value})', fontsize=14)\n",
    "    plt.savefig(f'modified_knn_strategy3_k{k_value}_{model_desc.replace(\" \",\"_\")}_robustness.png')\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    boxplot_labels = ['No Noise'] + [f'Noise {n:.2f}' for n in noise_levels]\n",
    "    plt.subplot(2, 1, 1);\n",
    "    valid_boxplot_acc = [d for d in ([bootstrap_original_acc] + bootstrap_acc_all) if d]\n",
    "    if valid_boxplot_acc: plt.boxplot(valid_boxplot_acc, labels=boxplot_labels[:len(valid_boxplot_acc)], showfliers=False)\n",
    "    plt.title(f'Distribution of Accuracy Scores ({model_desc})'); plt.ylabel('Accuracy'); plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "    plt.subplot(2, 1, 2);\n",
    "    valid_boxplot_f1 = [d for d in ([bootstrap_original_f1] + bootstrap_f1_all) if d]\n",
    "    if valid_boxplot_f1: plt.boxplot(valid_boxplot_f1, labels=boxplot_labels[:len(valid_boxplot_f1)], showfliers=False)\n",
    "    plt.title(f'Distribution of F1 Scores ({model_desc})'); plt.ylabel('F1 Score'); plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'modified_knn_strategy3_k{k_value}_{model_desc.replace(\" \",\"_\")}_noise_distribution.png')\n",
    "    plt.close()\n",
    "\n",
    "    logging.info(f\"\\n===== NOISE ROBUSTNESS RESULTS ({model_desc}) =====\")\n",
    "    logging.info(f\"{'Noise Level':<12} {'Mean Acc':<12} {'Mean F1':<12} {'p-val (Acc)':<12} {'p-val (F1)':<12}\")\n",
    "    acc_val_str = f\"{acc_values[0]:.7f}\" if not np.isnan(acc_values[0]) else \"NaN\"\n",
    "    f1_val_str = f\"{f1_values_plot[0]:.7f}\" if not np.isnan(f1_values_plot[0]) else \"NaN\"\n",
    "    logging.info(f\"{noise_labels[0]:<12} {acc_val_str:<12} {f1_val_str:<12} -            -\")\n",
    "    for i in range(len(noise_levels)):\n",
    "        p_acc_str = f\"{p_values_acc[i]:.2e}\" if not np.isnan(p_values_acc[i]) else \"N/A\"\n",
    "        p_f1_str = f\"{p_values_f1[i]:.2e}\" if not np.isnan(p_values_f1[i]) else \"N/A\"\n",
    "        acc_val_str = f\"{acc_values[i+1]:.7f}\" if not np.isnan(acc_values[i+1]) else \"NaN\"\n",
    "        f1_val_str = f\"{f1_values_plot[i+1]:.7f}\" if not np.isnan(f1_values_plot[i+1]) else \"NaN\"\n",
    "        logging.info(f\"{noise_labels[i+1]:<12} {acc_val_str:<12} {f1_val_str:<12} {p_acc_str:<12} {p_f1_str:<12}\")\n",
    "    return results, p_values_acc, p_values_f1\n",
    "\n",
    "def analyze_results(model_for_cm=None, X_cm=None, y_cm=None, model_desc=\"Test Model\"):\n",
    "    if not all_individual_accuracies:\n",
    "        logging.warning(\"analyze_results: No model results found to calculate.\")\n",
    "        acc_mean, acc_std, prec_mean, prec_std, rec_mean, rec_std, f1_mean, f1_std, time_mean, time_std = [np.nan]*10\n",
    "    else:\n",
    "        acc_mean, acc_std = np.mean(all_individual_accuracies), np.std(all_individual_accuracies)\n",
    "        prec_mean, prec_std = np.mean(all_individual_precisions), np.std(all_individual_precisions)\n",
    "        rec_mean, rec_std = np.mean(all_individual_recalls), np.std(all_individual_recalls)\n",
    "        f1_mean, f1_std = np.mean(all_individual_f1_scores), np.std(all_individual_f1_scores)\n",
    "        time_mean, time_std = np.mean(all_individual_times), np.std(all_individual_times)\n",
    "\n",
    "    logging.info(\"\\n===== OVERALL RESULTS (Averaged over all N_OUTER_SPLITS * N_INNER_SPLITS models) =====\")\n",
    "    logging.info(f\"k value (n_neighbors): {k_value}\")\n",
    "    logging.info(f\"Number of inner splits per outer fold: {N_INNER_SPLITS}\")\n",
    "    logging.info(f\"Total individual models evaluated: {len(all_individual_accuracies)}\")\n",
    "    logging.info(f\"Accuracy: {acc_mean:.7f} ± {acc_std:.7f}\")\n",
    "    logging.info(f\"Precision: {prec_mean:.7f} ± {prec_std:.7f}\")\n",
    "    logging.info(f\"Recall: {rec_mean:.7f} ± {rec_std:.7f}\")\n",
    "    logging.info(f\"F1 Score: {f1_mean:.7f} ± {f1_std:.7f}\")\n",
    "    logging.info(f\"Average execution time per inner model: {time_mean:.7f}s ± {time_std:.7f}s\")\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "    values = [acc_mean, prec_mean, rec_mean, f1_mean]\n",
    "    errors = [acc_std, prec_std, rec_std, f1_std]\n",
    "    bars = plt.bar(metrics, values, yerr=errors, capsize=10)\n",
    "    plt.title(f'Overall Performance (Avg. of Inner Models, k={k_value}, Inner Splits={N_INNER_SPLITS})')\n",
    "    plt.ylabel('Score'); plt.ylim(0, 1.1)\n",
    "    for bar, val, err in zip(bars, values, errors):\n",
    "        if not np.isnan(val) and not np.isnan(err):\n",
    "            plt.text(bar.get_x() + bar.get_width()/2., val + err + 0.02 if val + err < 1.05 else 1.05,\n",
    "                    f'{val:.7f}±{err:.7f}', ha='center', va='bottom', rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'modified_knn_strategy3_k{k_value}_n{N_INNER_SPLITS}_overall_metrics.png')\n",
    "    plt.close()\n",
    "\n",
    "    if model_for_cm and X_cm is not None and y_cm is not None and len(y_cm) > 0:\n",
    "        logging.info(f\"analyze_results CM for {model_desc}: X_cm shape: {X_cm.shape}, y_cm shape: {y_cm.shape}\")\n",
    "        cm_predictions = model_for_cm.predict(X_cm)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        cm = confusion_matrix(y_cm, cm_predictions)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "        plt.title(f'Confusion Matrix ({model_desc}, k={k_value})')\n",
    "        plt.xlabel('Predicted'); plt.ylabel('Actual')\n",
    "        plt.savefig(f'modified_knn_strategy3_k{k_value}_n{N_INNER_SPLITS}_{model_desc.replace(\" \",\"_\")}_CM.png')\n",
    "        plt.close()\n",
    "    elif not model_for_cm: logging.warning(f\"analyze_results: Model ({model_desc}) not found for CM.\")\n",
    "    else: logging.warning(f\"analyze_results: X_cm or y_cm for CM ({model_desc}) is missing/empty.\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    global all_individual_accuracies, all_individual_precisions, all_individual_recalls\n",
    "    global all_individual_f1_scores, all_individual_times, all_inner_model_details\n",
    "\n",
    "    all_individual_accuracies.clear(); all_individual_precisions.clear(); all_individual_recalls.clear()\n",
    "    all_individual_f1_scores.clear(); all_individual_times.clear(); all_inner_model_details.clear()\n",
    "\n",
    "    overall_start_time = time.time()\n",
    "    logging.info(f\"Modified KNN testing begins (Strateji 3: Strategy 3: Typical Internal Model) (k={k_value}, N_INNER_SPLITS={N_INNER_SPLITS})...\")\n",
    "\n",
    "    fold_counter = 0\n",
    "    for train_index, test_index in kf.split(X, y):\n",
    "        fold_counter += 1\n",
    "        logging.info(f\"\\n--- Outer Fold {fold_counter}/{N_OUTER_SPLITS} ---\")\n",
    "        X_train_full_orig, X_test_orig_fold = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train_full_orig, y_test_orig_fold = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train_full_scaled = scaler.fit_transform(X_train_full_orig)\n",
    "        X_test_scaled_fold = scaler.transform(X_test_orig_fold)\n",
    "        current_outer_fold_accuracies = []\n",
    "        num_samples_in_full_train = len(X_train_full_scaled)\n",
    "        actual_inner_splits = N_INNER_SPLITS\n",
    "        subset_size = num_samples_in_full_train // actual_inner_splits\n",
    "\n",
    "        if num_samples_in_full_train < N_INNER_SPLITS:\n",
    "            logging.warning(f\"Outer Fold {fold_counter}: Training data size ({num_samples_in_full_train}) is smaller than N_INNER_SPLITS ({N_INNER_SPLITS}). Reducing N_INNER_SPLITS to {num_samples_in_full_train}(each sample is a split).\")\n",
    "            actual_inner_splits = num_samples_in_full_train if num_samples_in_full_train > 0 else 1\n",
    "            subset_size = 1\n",
    "\n",
    "        if subset_size == 0 and actual_inner_splits > 0 and num_samples_in_full_train > 0 :\n",
    "             logging.warning(f\"Outer Fold {fold_counter}: Subset size is 0, num_samples_in_full_train={num_samples_in_full_train}, actual_inner_splits={actual_inner_splits}. Setting number of splits to 1.\")\n",
    "             actual_inner_splits = 1\n",
    "             subset_size = num_samples_in_full_train\n",
    "        elif num_samples_in_full_train == 0:\n",
    "            logging.warning(f\"Outer Fold {fold_counter}: Full training set is empty. Skipping this fold.\")\n",
    "            continue\n",
    "\n",
    "\n",
    "        for i in range(actual_inner_splits):\n",
    "            start_index = i * subset_size\n",
    "            end_index = (i + 1) * subset_size if i < actual_inner_splits - 1 else num_samples_in_full_train\n",
    "            if start_index >= end_index : continue\n",
    "\n",
    "\n",
    "            X_train_subset = X_train_full_scaled[start_index:end_index]\n",
    "            y_train_subset = y_train_full_orig.iloc[start_index:end_index]\n",
    "\n",
    "            if len(X_train_subset) == 0:\n",
    "                logging.warning(f\"Outer Fold {fold_counter}, Inner Split {i+1}: Training subset is empty. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            n_neighbors_actual = min(k_value, len(X_train_subset))\n",
    "            if n_neighbors_actual == 0 :\n",
    "                logging.warning(f\"Outer Fold {fold_counter}, Inner Split {i+1}: n_neighbors_actual became 0. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            unique_classes_in_subset = np.unique(y_train_subset)\n",
    "            if len(unique_classes_in_subset) < 2 and len(y_train_subset) > 0 :\n",
    "                 logging.warning(f\"Outer Fold {fold_counter}, Inner Split {i+1}: Training subset has only one class ({unique_classes_in_subset}). KNN might make constant predictions.\")\n",
    "                 if len(y_train_subset) < n_neighbors_actual and len(y_train_subset) > 0:\n",
    "                     n_neighbors_actual = len(y_train_subset)\n",
    "\n",
    "\n",
    "            inner_model_start_time = time.time()\n",
    "            knn = KNeighborsClassifier(n_neighbors=n_neighbors_actual)\n",
    "\n",
    "            try:\n",
    "                knn.fit(X_train_subset, y_train_subset)\n",
    "                if not hasattr(knn, 'classes_') or len(knn.classes_) < 2 :\n",
    "                    logging.warning(f\"Outer Fold {fold_counter}, Inner Split {i+1}: KNN model learned only one class. Predictions might be uniform.\")\n",
    "\n",
    "                predictions = knn.predict(X_test_scaled_fold)\n",
    "                acc = accuracy_score(y_test_orig_fold, predictions)\n",
    "                prec = precision_score(y_test_orig_fold, predictions, average='weighted', zero_division=0)\n",
    "                rec = recall_score(y_test_orig_fold, predictions, average='weighted', zero_division=0)\n",
    "                f1 = f1_score(y_test_orig_fold, predictions, average='weighted', zero_division=0)\n",
    "                inner_model_exec_time = time.time() - inner_model_start_time\n",
    "\n",
    "                all_individual_accuracies.append(acc); all_individual_precisions.append(prec)\n",
    "                all_individual_recalls.append(rec); all_individual_f1_scores.append(f1)\n",
    "                all_individual_times.append(inner_model_exec_time)\n",
    "                current_outer_fold_accuracies.append(acc)\n",
    "\n",
    "                all_inner_model_details.append({\n",
    "                    'model_object': knn, 'accuracy': acc, 'precision': prec, 'recall': rec, 'f1_score': f1,\n",
    "                    'X_test_scaled_fold': X_test_scaled_fold, 'y_test_orig_fold': y_test_orig_fold,\n",
    "                    'fold_num': fold_counter, 'inner_split_num': i + 1, 'train_subset_size': len(X_train_subset)\n",
    "                })\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Outer Fold {fold_counter}, Inner Split {i+1}: Error during model training/prediction: {e}\")\n",
    "\n",
    "        if current_outer_fold_accuracies:\n",
    "            mean_acc_fold, std_acc_fold = np.mean(current_outer_fold_accuracies), np.std(current_outer_fold_accuracies)\n",
    "            logging.info(f\"Outer Fold {fold_counter} Avg. Acc of Inner Models: {mean_acc_fold:.7f} (Std: {std_acc_fold:.7f})\")\n",
    "        else:\n",
    "            logging.info(f\"Outer Fold {fold_counter}: No valid inner model could be evaluated in this fold.\")\n",
    "\n",
    "    overall_end_time = time.time()\n",
    "    logging.info(f\"\\nAll outer folds and inner splits completed. Total time: {overall_end_time - overall_start_time:.2f} seconds\")\n",
    "\n",
    "    model_for_analysis = None\n",
    "    X_test_for_analysis = None\n",
    "    y_test_for_analysis = None\n",
    "    model_description_for_analysis = \"N/A\"\n",
    "\n",
    "    if all_inner_model_details:\n",
    "        if all_individual_accuracies:\n",
    "            mean_overall_accuracy = np.mean(all_individual_accuracies)\n",
    "            logging.info(f\"Average accuracy of all inner models (for typical model selection): {mean_overall_accuracy:.7f}\")\n",
    "\n",
    "            valid_models_for_typical_selection = [m for m in all_inner_model_details if not np.isnan(m['accuracy'])]\n",
    "            if valid_models_for_typical_selection:\n",
    "                typical_inner_model_info = min(valid_models_for_typical_selection, key=lambda item: abs(item['accuracy'] - mean_overall_accuracy))\n",
    "                model_for_analysis = typical_inner_model_info['model_object']\n",
    "                X_test_for_analysis = typical_inner_model_info['X_test_scaled_fold']\n",
    "                y_test_for_analysis = typical_inner_model_info['y_test_orig_fold']\n",
    "                model_description_for_analysis = \"Typical Inner Model\"\n",
    "\n",
    "                logging.info(f\"\\nIndividual INNER MODEL closest to the average (TYPICAL) selected (for Noise Test and CM):\")\n",
    "                logging.info(f\"  Originating Outer Fold: {typical_inner_model_info['fold_num']}\")\n",
    "                logging.info(f\"  Originating Inner Split: {typical_inner_model_info['inner_split_num']}\")\n",
    "                logging.info(f\"  Training Subset Size: {typical_inner_model_info['train_subset_size']}\")\n",
    "                logging.info(f\"  Accuracy of this Model (on its respective outer fold test set): {typical_inner_model_info['accuracy']:.7f}\")\n",
    "                logging.info(f\"  This Model's F1 Score: {typical_inner_model_info['f1_score']:.7f}\")\n",
    "            else:\n",
    "                logging.warning(\"A typical inner model close to the average could not be selected (no valid models).\")\n",
    "        else:\n",
    "            logging.warning(\"Average accuracy could not be calculated, a typical model cannot be selected. The best model will be tried as a fallback.\")\n",
    "            if all_inner_model_details:\n",
    "                 best_fallback_model_info = max(all_inner_model_details, key=lambda item: item['accuracy'])\n",
    "                 model_for_analysis = best_fallback_model_info['model_object']\n",
    "                 X_test_for_analysis = best_fallback_model_info['X_test_scaled_fold']\n",
    "                 y_test_for_analysis = best_fallback_model_info['y_test_orig_fold']\n",
    "                 model_description_for_analysis = \"Best Inner Model (Fallback)\"\n",
    "                 logging.info(f\"\\nFALLBACK: Best Inner Model selected.\")\n",
    "                 logging.info(f\"  Accuracy: {best_fallback_model_info['accuracy']:.7f}\")\n",
    "\n",
    "\n",
    "    else:\n",
    "        logging.warning(\"No inner model details were recorded. A model could not be selected for Noise Test and CM.\")\n",
    "\n",
    "    analyze_results(model_for_cm=model_for_analysis,\n",
    "                    X_cm=X_test_for_analysis,\n",
    "                    y_cm=y_test_for_analysis,\n",
    "                    model_desc=model_description_for_analysis)\n",
    "\n",
    "    if model_for_analysis and X_test_for_analysis is not None and y_test_for_analysis is not None and len(y_test_for_analysis) > 0 :\n",
    "        logging.info(f\"\\nRunning noise robustness test for the selected {model_description_for_analysis}...\")\n",
    "        noise_levels_config = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "        n_bootstrap_config = 100\n",
    "        evaluate_robustness(\n",
    "            model_for_analysis, X_test_for_analysis, y_test_for_analysis,\n",
    "            noise_levels=noise_levels_config, n_bootstrap=n_bootstrap_config,\n",
    "            model_desc=model_description_for_analysis\n",
    "        )\n",
    "    elif not model_for_analysis:\n",
    "        logging.info(f\"A suitable model ({model_description_for_analysis}) for the noise test could not be found.\")\n",
    "    else:\n",
    "        logging.info(f\"For the noise test, {model_description_for_analysis}'s test data is missing/empty.\")\n",
    "\n",
    "    logging.info(\"Process completed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "90313d088050ae0c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Fed3C (Bayesian Optimization)",
   "id": "7b28fdc5fe36e468"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T08:12:12.822059Z",
     "start_time": "2025-05-18T08:12:12.818958Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import argparse\n",
    "from scipy.io import arff\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Integer\n",
    "from skopt.utils import use_named_args\n",
    "import logging\n",
    "from joblib import Parallel, delayed\n",
    "from scipy import stats\n",
    "\n",
    "args = type('', (), {})()\n",
    "args.num_clients = 10\n",
    "args.max_centroids = None\n",
    "args.n_calls = 50\n",
    "args.k = 1\n",
    "args.experiment_name = \"k1_rice_reference_experiment\"\n",
    "\n",
    "if args.experiment_name is None:\n",
    "    exp_name = f\"clients_{args.num_clients}_opt_{args.n_calls}_k_{args.k}\"\n",
    "    if args.max_centroids is not None:\n",
    "        exp_name += f\"_maxcentroid_{args.max_centroids}\"\n",
    "    args.experiment_name = exp_name\n",
    "\n",
    "RESULTS_ROOT = \"results\"\n",
    "RESULTS_DIR = os.path.join(RESULTS_ROOT, args.experiment_name)\n",
    "\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "log_file_path = os.path.join(RESULTS_DIR, 'rice_bayesian.log')\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "                    handlers=[logging.FileHandler(log_file_path), logging.StreamHandler()])\n",
    "\n",
    "logging.info(f\"===== EXPERIMENT CONFIGURATION =====\")\n",
    "logging.info(f\"Experiment Name: {args.experiment_name}\")\n",
    "logging.info(f\"Number of Clients: {args.num_clients}\")\n",
    "logging.info(f\"Optimization Calls: {args.n_calls}\")\n",
    "logging.info(f\"KNN n_neighbors (k): {args.k}\")\n",
    "logging.info(f\"Max Centroids: {'Half of training data' if args.max_centroids is None else args.max_centroids}\")\n",
    "logging.info(f\"Results Directory: {RESULTS_DIR}\")\n",
    "\n",
    "data_path = 'rice/Rice_Cammeo_Osmancik.arff'\n",
    "data, meta = arff.loadarff(data_path)\n",
    "data_df = pd.DataFrame(data)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "data_df['Class'] = label_encoder.fit_transform(data_df['Class'].astype(str))\n",
    "\n",
    "X = data_df.drop(columns=['Class'])\n",
    "y = data_df['Class']\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "total_data = len(data_df)\n",
    "train_data_per_fold = total_data * (4/5)\n",
    "\n",
    "if args.max_centroids is not None:\n",
    "    max_centroids = args.max_centroids\n",
    "else:\n",
    "    max_centroids = int(train_data_per_fold / 2)\n",
    "\n",
    "logging.info(f\"Calculated Max Centroids: {max_centroids}\")\n",
    "\n",
    "space = [Integer(10, max_centroids, name='total_centroids')]\n",
    "\n",
    "best_accuracy = float('inf')\n",
    "best_centroids = None\n",
    "best_accuracies = []\n",
    "best_precisions = []\n",
    "best_recalls = []\n",
    "best_f1s = []\n",
    "best_times = []\n",
    "best_client_times = []\n",
    "best_comm_costs = []\n",
    "best_y_test = None\n",
    "best_predictions = None\n",
    "\n",
    "all_centroids = []\n",
    "all_accuracies = []\n",
    "all_precisions = []\n",
    "all_recalls = []\n",
    "all_f1s = []\n",
    "all_times = []\n",
    "all_client_times = []\n",
    "all_comm_costs = []\n",
    "\n",
    "def log_details(split_data, centroids_per_split):\n",
    "    for i, (data, centroids) in enumerate(zip(split_data, centroids_per_split)):\n",
    "        logging.info(f\"Client {i+1}: Data count = {len(data)}, Number of assigned centroids = {int(centroids)}\")\n",
    "\n",
    "def distribute_centroids(n_data, total_centroids):\n",
    "    proportions = n_data / np.sum(n_data)\n",
    "    floored_centroids = np.floor(proportions * total_centroids).astype(int)\n",
    "    remainder = total_centroids - np.sum(floored_centroids)\n",
    "\n",
    "    while remainder > 0:\n",
    "        idx = np.argmax(proportions - floored_centroids / total_centroids)\n",
    "        floored_centroids[idx] += 1\n",
    "        remainder -= 1\n",
    "\n",
    "    return floored_centroids\n",
    "\n",
    "def calculate_communication_cost(centroids_per_split, feature_dim):\n",
    "    bytes_per_centroid = feature_dim * 8\n",
    "    total_bytes = sum(centroids_per_split) * bytes_per_centroid\n",
    "    return total_bytes / 1024\n",
    "\n",
    "def process_fold(train_index, test_index, total_centroids, X, y):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X.iloc[train_index])\n",
    "    X_test_scaled = scaler.transform(X.iloc[test_index])\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    num_splits = args.num_clients\n",
    "    split_train_data = np.array_split(X_train_scaled, num_splits)\n",
    "    split_train_labels = np.array_split(y_train, num_splits)\n",
    "    data_counts = np.array([len(data) for data in split_train_data])\n",
    "    centroids_per_split = distribute_centroids(data_counts, total_centroids)\n",
    "\n",
    "    center_labels = {}\n",
    "    for split_data, split_labels, centroids_count in zip(split_train_data, split_train_labels, centroids_per_split):\n",
    "        unique_labels, counts = np.unique(split_labels, return_counts=True)\n",
    "        label_counts = dict(zip(unique_labels, counts / counts.sum()))\n",
    "        centroids_per_class = {label: max(1, int(count * centroids_count))\n",
    "                              for label, count in label_counts.items()}\n",
    "\n",
    "        total_class_centroids = sum(centroids_per_class.values())\n",
    "        if total_class_centroids > centroids_count:\n",
    "            scale_factor = centroids_count / total_class_centroids\n",
    "            centroids_per_class = {label: max(1, int(count * scale_factor))\n",
    "                                  for label, count in centroids_per_class.items()}\n",
    "\n",
    "        for label in unique_labels:\n",
    "            class_data = split_data[split_labels == label]\n",
    "            if centroids_per_class[label] > 0 and len(class_data) > 0:\n",
    "                kmeans = KMeans(n_clusters=min(len(class_data), centroids_per_class[label]), random_state=42)\n",
    "                kmeans.fit(class_data)\n",
    "                if label in center_labels:\n",
    "                    center_labels[label].extend(kmeans.cluster_centers_)\n",
    "                else:\n",
    "                    center_labels[label] = list(kmeans.cluster_centers_)\n",
    "\n",
    "    final_centers = [center for sublist in center_labels.values() for center in sublist]\n",
    "    final_labels = [label for label, centers in center_labels.items() for center in centers]\n",
    "\n",
    "    knn = KNeighborsClassifier(n_neighbors=args.k)\n",
    "    knn.fit(final_centers, final_labels)\n",
    "    predictions = knn.predict(X_test_scaled)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    precision = precision_score(y_test, predictions, average='weighted')\n",
    "    recall = recall_score(y_test, predictions, average='weighted')\n",
    "    f1 = f1_score(y_test, predictions, average='weighted')\n",
    "\n",
    "    end_time = time.time()\n",
    "    exec_time = end_time - start_time\n",
    "\n",
    "    feature_dim = X.shape[1]\n",
    "    comm_cost = calculate_communication_cost(centroids_per_split, feature_dim)\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'time': exec_time,\n",
    "        'comm_cost': comm_cost,\n",
    "        'y_test': y_test,\n",
    "        'predictions': predictions\n",
    "    }\n",
    "\n",
    "@use_named_args(space)\n",
    "def objective_sequential(total_centroids):\n",
    "    logging.info(f\"Total number of centroids tried = {total_centroids}\")\n",
    "\n",
    "    fold_accuracies = []\n",
    "    fold_precisions = []\n",
    "    fold_recalls = []\n",
    "    fold_f1s = []\n",
    "    fold_times = []\n",
    "    fold_client_times = []\n",
    "    fold_comm_costs = []\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        fold_start_time = time.time()\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X.iloc[train_index])\n",
    "        X_test_scaled = scaler.transform(X.iloc[test_index])\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        num_splits = args.num_clients\n",
    "        split_train_data = np.array_split(X_train_scaled, num_splits)\n",
    "        split_train_labels = np.array_split(y_train, num_splits)\n",
    "        data_counts = np.array([len(data) for data in split_train_data])\n",
    "        centroids_per_split = distribute_centroids(data_counts, total_centroids)\n",
    "\n",
    "        center_labels = {}\n",
    "        client_times = []\n",
    "\n",
    "        for split_data, split_labels, centroids_count in zip(split_train_data, split_train_labels, centroids_per_split):\n",
    "            client_start_time = time.time()\n",
    "\n",
    "            unique_labels, counts = np.unique(split_labels, return_counts=True)\n",
    "            label_counts = dict(zip(unique_labels, counts / counts.sum()))\n",
    "            centroids_per_class = {label: max(1, int(count * centroids_count))\n",
    "                                  for label, count in label_counts.items()}\n",
    "\n",
    "            total_class_centroids = sum(centroids_per_class.values())\n",
    "            if total_class_centroids > centroids_count:\n",
    "                scale_factor = centroids_count / total_class_centroids\n",
    "                centroids_per_class = {label: max(1, int(count * scale_factor))\n",
    "                                      for label, count in centroids_per_class.items()}\n",
    "\n",
    "            for label in unique_labels:\n",
    "                class_data = split_data[split_labels == label]\n",
    "                if centroids_per_class[label] > 0 and len(class_data) > 0:\n",
    "                    kmeans = KMeans(n_clusters=min(len(class_data), centroids_per_class[label]), random_state=42)\n",
    "                    kmeans.fit(class_data)\n",
    "                    if label in center_labels:\n",
    "                        center_labels[label].extend(kmeans.cluster_centers_)\n",
    "                    else:\n",
    "                        center_labels[label] = list(kmeans.cluster_centers_)\n",
    "\n",
    "            client_end_time = time.time()\n",
    "            client_times.append(client_end_time - client_start_time)\n",
    "\n",
    "        final_centers = [center for sublist in center_labels.values() for center in sublist]\n",
    "        final_labels = [label for label, centers in center_labels.items() for center in centers]\n",
    "\n",
    "        knn = KNeighborsClassifier(n_neighbors=args.k)\n",
    "        knn.fit(final_centers, final_labels)\n",
    "        predictions = knn.predict(X_test_scaled)\n",
    "\n",
    "        accuracy = accuracy_score(y_test, predictions)\n",
    "        precision = precision_score(y_test, predictions, average='weighted')\n",
    "        recall = recall_score(y_test, predictions, average='weighted')\n",
    "        f1 = f1_score(y_test, predictions, average='weighted')\n",
    "\n",
    "        fold_end_time = time.time()\n",
    "        fold_exec_time = fold_end_time - fold_start_time\n",
    "\n",
    "        feature_dim = X.shape[1]\n",
    "        comm_cost = calculate_communication_cost(centroids_per_split, feature_dim)\n",
    "\n",
    "        fold_accuracies.append(accuracy)\n",
    "        fold_precisions.append(precision)\n",
    "        fold_recalls.append(recall)\n",
    "        fold_f1s.append(f1)\n",
    "        fold_times.append(fold_exec_time)\n",
    "        fold_client_times.append(client_times)\n",
    "        fold_comm_costs.append(comm_cost)\n",
    "\n",
    "    mean_accuracy = np.mean(fold_accuracies)\n",
    "    mean_precision = np.mean(fold_precisions)\n",
    "    mean_recall = np.mean(fold_recalls)\n",
    "    mean_f1 = np.mean(fold_f1s)\n",
    "    mean_time = np.mean(fold_times)\n",
    "    mean_client_time = np.mean([np.mean(client_times) for client_times in fold_client_times])\n",
    "    mean_comm_cost = np.mean(fold_comm_costs)\n",
    "\n",
    "    global best_accuracy, best_centroids, best_accuracies, best_precisions, best_recalls, best_f1s, best_times, best_client_times, best_comm_costs, best_y_test, best_predictions\n",
    "\n",
    "    all_centroids.append(total_centroids)\n",
    "    all_accuracies.append(mean_accuracy)\n",
    "    all_precisions.append(mean_precision)\n",
    "    all_recalls.append(mean_recall)\n",
    "    all_f1s.append(mean_f1)\n",
    "    all_times.append(mean_time)\n",
    "    all_client_times.append(mean_client_time)\n",
    "    all_comm_costs.append(mean_comm_cost)\n",
    "\n",
    "    if -mean_accuracy < best_accuracy:\n",
    "        best_accuracy = -mean_accuracy\n",
    "        best_centroids = total_centroids\n",
    "        best_accuracies = fold_accuracies\n",
    "        best_precisions = fold_precisions\n",
    "        best_recalls = fold_recalls\n",
    "        best_f1s = fold_f1s\n",
    "        best_times = fold_times\n",
    "        best_client_times = [np.mean(client_times) for client_times in fold_client_times]\n",
    "        best_comm_costs = fold_comm_costs\n",
    "\n",
    "        # Test\n",
    "        if len(fold_accuracies) > 0:\n",
    "            best_fold_idx = np.argmax(fold_accuracies)\n",
    "            best_fold_indices = list(kf.split(X))[best_fold_idx]\n",
    "            test_indices = best_fold_indices[1]\n",
    "            best_y_test = y.iloc[test_indices]\n",
    "\n",
    "            best_X_test = X.iloc[test_indices]\n",
    "            best_X_test_scaled = scaler.transform(best_X_test)\n",
    "            best_predictions = knn.predict(best_X_test_scaled)\n",
    "\n",
    "    logging.info(f\"Centroids: {total_centroids}, Acc: {mean_accuracy:.7f}, Prec: {mean_precision:.7f}, \" +\n",
    "                 f\"Rec: {mean_recall:.7f}, F1: {mean_f1:.7f}, Time (fold): {mean_time:.2f}s, \" +\n",
    "                 f\"Time (client): {mean_client_time:.7f}s, Comm: {mean_comm_cost:.2f}KB\")\n",
    "\n",
    "    return -mean_accuracy\n",
    "\n",
    "def evaluate_robustness(best_model, X_test, y_test, noise_levels=[0.1, 0.2, 0.3, 0.4, 0.5], n_bootstrap=100):\n",
    "    results = []\n",
    "\n",
    "    n_samples = len(y_test)\n",
    "\n",
    "    original_pred = best_model.predict(X_test)\n",
    "    original_acc = accuracy_score(y_test, original_pred)\n",
    "    original_f1 = f1_score(y_test, original_pred, average='weighted')\n",
    "    results.append(('No Noise', original_acc, original_f1))\n",
    "\n",
    "    bootstrap_original_acc = []\n",
    "    bootstrap_original_f1 = []\n",
    "\n",
    "    for _ in range(n_bootstrap):\n",
    "        indices = np.random.choice(n_samples, n_samples, replace=True)\n",
    "        X_boot = X_test[indices]\n",
    "        y_boot = y_test.iloc[indices] if hasattr(y_test, 'iloc') else y_test[indices]\n",
    "\n",
    "        boot_pred = best_model.predict(X_boot)\n",
    "        bootstrap_original_acc.append(accuracy_score(y_boot, boot_pred))\n",
    "        bootstrap_original_f1.append(f1_score(y_boot, boot_pred, average='weighted'))\n",
    "\n",
    "    p_values_acc = []\n",
    "    p_values_f1 = []\n",
    "    bootstrap_acc_all = []\n",
    "    bootstrap_f1_all = []\n",
    "\n",
    "    for noise in noise_levels:\n",
    "        bootstrap_acc = []\n",
    "        bootstrap_f1 = []\n",
    "\n",
    "        for _ in range(n_bootstrap):\n",
    "            indices = np.random.choice(n_samples, n_samples, replace=True)\n",
    "            X_boot = X_test[indices].copy()\n",
    "            y_boot = y_test.iloc[indices] if hasattr(y_test, 'iloc') else y_test[indices]\n",
    "\n",
    "            noise_matrix = np.random.normal(0, noise, X_boot.shape)\n",
    "            X_boot += noise_matrix\n",
    "\n",
    "            noisy_pred = best_model.predict(X_boot)\n",
    "            noisy_acc = accuracy_score(y_boot, noisy_pred)\n",
    "            noisy_f1 = f1_score(y_boot, noisy_pred, average='weighted')\n",
    "\n",
    "            bootstrap_acc.append(noisy_acc)\n",
    "            bootstrap_f1.append(noisy_f1)\n",
    "\n",
    "        mean_acc = np.mean(bootstrap_acc)\n",
    "        mean_f1 = np.mean(bootstrap_f1)\n",
    "\n",
    "        t_stat_acc, p_val_acc = stats.ttest_rel(bootstrap_original_acc, bootstrap_acc)\n",
    "        t_stat_f1, p_val_f1 = stats.ttest_rel(bootstrap_original_f1, bootstrap_f1)\n",
    "\n",
    "        results.append((f'Noise {noise:.2f}', mean_acc, mean_f1))\n",
    "        p_values_acc.append(p_val_acc)\n",
    "        p_values_f1.append(p_val_f1)\n",
    "        bootstrap_acc_all.append(bootstrap_acc)\n",
    "        bootstrap_f1_all.append(bootstrap_f1)\n",
    "\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    noise_labels = [r[0] for r in results]\n",
    "    acc_values = [r[1] for r in results]\n",
    "    f1_values = [r[2] for r in results]\n",
    "\n",
    "    x = np.arange(len(noise_labels))\n",
    "    width = 0.35\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    bars_acc = plt.bar(x - width/2, acc_values, width, label='Accuracy')\n",
    "    bars_f1 = plt.bar(x + width/2, f1_values, width, label='F1 Score')\n",
    "    plt.xlabel('Noise Level')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Model Robustness to Noise')\n",
    "    plt.xticks(x, noise_labels, rotation=45)\n",
    "    plt.legend()\n",
    "\n",
    "    for i in range(len(p_values_acc)):\n",
    "        if i > 0:\n",
    "            if p_values_acc[i-1] < 0.001:\n",
    "                plt.text(i - width/2, acc_values[i] - 0.02, '***', ha='center')\n",
    "            elif p_values_acc[i-1] < 0.01:\n",
    "                plt.text(i - width/2, acc_values[i] - 0.02, '**', ha='center')\n",
    "            elif p_values_acc[i-1] < 0.05:\n",
    "                plt.text(i - width/2, acc_values[i] - 0.02, '*', ha='center')\n",
    "\n",
    "            if p_values_f1[i-1] < 0.001:\n",
    "                plt.text(i + width/2, f1_values[i] - 0.02, '***', ha='center')\n",
    "            elif p_values_f1[i-1] < 0.01:\n",
    "                plt.text(i + width/2, f1_values[i] - 0.02, '**', ha='center')\n",
    "            elif p_values_f1[i-1] < 0.05:\n",
    "                plt.text(i + width/2, f1_values[i] - 0.02, '*', ha='center')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    bars_acc = plt.bar(x - width/2, acc_values, width, label='Accuracy')\n",
    "    bars_f1 = plt.bar(x + width/2, f1_values, width, label='F1 Score')\n",
    "    plt.xlabel('Noise Level')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Zoomed View (0.85-1.0 range) with p-values')\n",
    "    plt.xticks(x, noise_labels, rotation=45)\n",
    "    plt.ylim(0.85, 1.0)\n",
    "\n",
    "    for i in range(len(p_values_acc)):\n",
    "        if i > 0:\n",
    "            if p_values_acc[i-1] < 0.001:\n",
    "                plt.text(i - width/2, acc_values[i] + 0.005, '***', ha='center', fontsize=10)\n",
    "            elif p_values_acc[i-1] < 0.01:\n",
    "                plt.text(i - width/2, acc_values[i] + 0.005, '**', ha='center', fontsize=10)\n",
    "            elif p_values_acc[i-1] < 0.05:\n",
    "                plt.text(i - width/2, acc_values[i] + 0.005, '*', ha='center', fontsize=10)\n",
    "\n",
    "            if p_values_f1[i-1] < 0.001:\n",
    "                plt.text(i + width/2, f1_values[i] + 0.005, '***', ha='center', fontsize=10)\n",
    "            elif p_values_f1[i-1] < 0.01:\n",
    "                plt.text(i + width/2, f1_values[i] + 0.005, '**', ha='center', fontsize=10)\n",
    "            elif p_values_f1[i-1] < 0.05:\n",
    "                plt.text(i + width/2, f1_values[i] + 0.005, '*', ha='center', fontsize=10)\n",
    "\n",
    "    for i, (acc, f1) in enumerate(zip(acc_values, f1_values)):\n",
    "        plt.text(i - width/2, acc - 0.01, f'{acc:.7f}', ha='center', va='bottom', fontsize=8, rotation=90)\n",
    "        plt.text(i + width/2, f1 - 0.01, f'{f1:.7f}', ha='center', va='bottom', fontsize=8, rotation=90)\n",
    "\n",
    "    plt.figtext(0.5, 0.01, \"* p<0.05, ** p<0.01, *** p<0.001\", ha=\"center\", fontsize=10)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(RESULTS_DIR, 'rice_noise_robustness.png'))\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(14, 8))\n",
    "\n",
    "    plt.subplot(2, 1, 1)\n",
    "    boxplot_data_acc = [bootstrap_original_acc] + bootstrap_acc_all\n",
    "    plt.boxplot(boxplot_data_acc, labels=noise_labels, showfliers=False)\n",
    "    plt.title('Distribution of Accuracy Scores Across Noise Levels')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    boxplot_data_f1 = [bootstrap_original_f1] + bootstrap_f1_all\n",
    "    plt.boxplot(boxplot_data_f1, labels=noise_labels, showfliers=False)\n",
    "    plt.title('Distribution of F1 Scores Across Noise Levels')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(RESULTS_DIR, 'rice_noise_distribution.png'))\n",
    "    plt.close()\n",
    "\n",
    "    logging.info(\"\\n===== NOISE ROBUSTNESS RESULTS WITH P-VALUES =====\")\n",
    "    logging.info(f\"{'Noise Level':<12} {'Accuracy':<15} {'F1 Score':<15} {'p-val (Acc)':<12} {'p-val (F1)':<12}\")\n",
    "\n",
    "    logging.info(f\"{noise_labels[0]:<12} {acc_values[0]:.7f}      {f1_values[0]:.7f}      -            -\")\n",
    "\n",
    "    for i in range(1, len(noise_labels)):\n",
    "        logging.info(f\"{noise_labels[i]:<12} {acc_values[i]:.7f}      {f1_values[i]:.7f}      {p_values_acc[i-1]:.2e}     {p_values_f1[i-1]:.2e}\")\n",
    "\n",
    "    return results, p_values_acc, p_values_f1\n",
    "\n",
    "def analyze_results():\n",
    "    acc_mean, acc_std = np.mean(best_accuracies), np.std(best_accuracies)\n",
    "    prec_mean, prec_std = np.mean(best_precisions), np.std(best_precisions)\n",
    "    rec_mean, rec_std = np.mean(best_recalls), np.std(best_recalls)\n",
    "    f1_mean, f1_std = np.mean(best_f1s), np.std(best_f1s)\n",
    "    time_mean, time_std = np.mean(best_times), np.std(best_times)\n",
    "    client_time_mean, client_time_std = np.mean(best_client_times), np.std(best_client_times)\n",
    "    comm_cost_mean, comm_cost_std = np.mean(best_comm_costs), np.std(best_comm_costs)\n",
    "\n",
    "    logging.info(\"\\n===== BEST CONFIGURATION RESULTS (CLASS-BALANCED CENTROID ALLOCATION) =====\")\n",
    "    logging.info(f\"Best number of centroids: {best_centroids}\")\n",
    "    logging.info(f\"Number of clients: {args.num_clients}\")\n",
    "    logging.info(f\"KNN n_neighbors (k): {args.k}\")\n",
    "    logging.info(f\"Centroid allocation strategy: Class-balanced (Allocation to each class proportional to its data size)\")\n",
    "    logging.info(f\"Accuracy: {acc_mean:.7f} ± {acc_std:.7f}\")\n",
    "    logging.info(f\"Precision: {prec_mean:.7f} ± {prec_std:.7f}\")\n",
    "    logging.info(f\"Recall: {rec_mean:.7f} ± {rec_std:.7f}\")\n",
    "    logging.info(f\"F1 Score: {f1_mean:.7f} ± {f1_std:.7f}\")\n",
    "    logging.info(f\"Average time per fold: {time_mean:.7f}s ± {time_std:.7f}s\")\n",
    "    logging.info(f\"Average time per client: {client_time_mean:.7f}s ± {client_time_std:.7f}s\")\n",
    "    logging.info(f\"Communication cost: {comm_cost_mean:.7f}KB ± {comm_cost_std:.7f}KB\")\n",
    "\n",
    "    if best_y_test is not None:\n",
    "        unique_labels, counts = np.unique(best_y_test, return_counts=True)\n",
    "        class_ratios = dict(zip(unique_labels, counts / counts.sum()))\n",
    "\n",
    "        for label, ratio in class_ratios.items():\n",
    "            class_name = label_encoder.inverse_transform([label])[0] if hasattr(label_encoder, 'inverse_transform') else f\"Class {label}\"\n",
    "            logging.info(f\"Class ratio ({class_name}): {ratio:.7f}\")\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "    values = [acc_mean, prec_mean, rec_mean, f1_mean]\n",
    "    errors = [acc_std, prec_std, rec_std, f1_std]\n",
    "\n",
    "    bars = plt.bar(metrics, values, yerr=errors, capsize=10)\n",
    "    plt.title(f'Performance Metrics with Error Bars (Centroids: {best_centroids}, Clients: {args.num_clients})')\n",
    "    plt.ylabel('Score')\n",
    "    plt.ylim(0, 1.1)\n",
    "\n",
    "    for bar, val, err in zip(bars, values, errors):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., val + err + 0.02,\n",
    "                f'{val:.7f}±{err:.7f}', ha='center', va='bottom', rotation=0)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(RESULTS_DIR, 'rice_performance_metrics.png'))\n",
    "    plt.close()\n",
    "\n",
    "    if best_y_test is not None and best_predictions is not None:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        cm = confusion_matrix(best_y_test, best_predictions)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.title(f'Confusion Matrix (Centroids: {best_centroids}, Clients: {args.num_clients})')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Actual')\n",
    "        plt.savefig(os.path.join(RESULTS_DIR, 'rice_confusion_matrix.png'))\n",
    "        plt.close()\n",
    "\n",
    "    plt.figure(figsize=(14, 8))\n",
    "\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(range(len(all_centroids)), all_accuracies, 'b-', label='Accuracy')\n",
    "    plt.plot(range(len(all_centroids)), all_f1s, 'g-', label='F1 Score')\n",
    "    plt.axvline(x=all_centroids.index(best_centroids), color='r', linestyle='--', label=f'Best Centroids={best_centroids}')\n",
    "    plt.title('Accuracy and F1 Score vs. Iteration')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(range(len(all_centroids)), all_precisions, 'm-', label='Precision')\n",
    "    plt.plot(range(len(all_centroids)), all_recalls, 'c-', label='Recall')\n",
    "    plt.axvline(x=all_centroids.index(best_centroids), color='r', linestyle='--', label=f'Best Centroids={best_centroids}')\n",
    "    plt.title('Precision and Recall vs. Iteration')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(range(len(all_centroids)), all_times, 'k-', label='Fold Time')\n",
    "    plt.plot(range(len(all_centroids)), all_client_times, 'y-', label='Client Time')\n",
    "    plt.axvline(x=all_centroids.index(best_centroids), color='r', linestyle='--', label=f'Best Centroids={best_centroids}')\n",
    "    plt.title('Execution Time vs. Iteration')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Time (s)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(range(len(all_centroids)), all_comm_costs, 'r-', label='Comm Cost')\n",
    "    plt.axvline(x=all_centroids.index(best_centroids), color='b', linestyle='--', label=f'Best Centroids={best_centroids}')\n",
    "    plt.title('Communication Cost vs. Iteration')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Communication Cost (KB)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(RESULTS_DIR, 'rice_optimization_progress.png'))\n",
    "    plt.close()\n",
    "\n",
    "    return {\n",
    "        'acc_mean': acc_mean, 'acc_std': acc_std,\n",
    "        'prec_mean': prec_mean, 'prec_std': prec_std,\n",
    "        'rec_mean': rec_mean, 'rec_std': rec_std,\n",
    "        'f1_mean': f1_mean, 'f1_std': f1_std,\n",
    "        'time_mean': time_mean, 'time_std': time_std,\n",
    "        'client_time_mean': client_time_mean, 'client_time_std': client_time_std,\n",
    "        'comm_cost_mean': comm_cost_mean, 'comm_cost_std': comm_cost_std\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    start_time = time.time()\n",
    "\n",
    "    logging.info(f\"Bayesian optimization starting (without parallelization, with class-balanced centroid allocation)...\")\n",
    "    logging.info(f\"Parameter configuration: Number of Clients={args.num_clients}, Max Centroid={max_centroids}, Optimization Iterations={args.n_calls}\")\n",
    "\n",
    "    res_gp = gp_minimize(objective_sequential, space, n_calls=args.n_calls, random_state=0)\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    logging.info(f\"Optimization completed. Total time: {total_time:.7f} seconds\")\n",
    "\n",
    "    metrics = analyze_results()\n",
    "\n",
    "    logging.info(\"Running noise robustness test...\")\n",
    "\n",
    "    best_fold_idx = np.argmax(best_accuracies)\n",
    "    train_indices = list(kf.split(X))[best_fold_idx][0]\n",
    "    test_indices = list(kf.split(X))[best_fold_idx][1]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X.iloc[train_indices])\n",
    "    X_test_scaled = scaler.transform(X.iloc[test_indices])\n",
    "    y_train, y_test = y.iloc[train_indices], y.iloc[test_indices]\n",
    "\n",
    "    client_start_time = time.time()\n",
    "\n",
    "    num_splits = args.num_clients\n",
    "    split_train_data = np.array_split(X_train_scaled, num_splits)\n",
    "    split_train_labels = np.array_split(y_train, num_splits)\n",
    "    data_counts = np.array([len(data) for data in split_train_data])\n",
    "    centroids_per_split = distribute_centroids(data_counts, best_centroids)\n",
    "\n",
    "    client_times = []\n",
    "\n",
    "    center_labels = {}\n",
    "    for split_data, split_labels, centroids_count in zip(split_train_data, split_train_labels, centroids_per_split):\n",
    "        client_iter_start = time.time()\n",
    "\n",
    "        unique_labels, counts = np.unique(split_labels, return_counts=True)\n",
    "        label_counts = dict(zip(unique_labels, counts / counts.sum()))\n",
    "        centroids_per_class = {label: max(1, int(count * centroids_count))\n",
    "                              for label, count in label_counts.items()}\n",
    "\n",
    "        total_class_centroids = sum(centroids_per_class.values())\n",
    "        if total_class_centroids > centroids_count:\n",
    "            scale_factor = centroids_count / total_class_centroids\n",
    "            centroids_per_class = {label: max(1, int(count * scale_factor))\n",
    "                                   for label, count in centroids_per_class.items()}\n",
    "\n",
    "        logging.info(f\"Centroid distribution: {centroids_per_class}\")\n",
    "\n",
    "        for label in unique_labels:\n",
    "            class_data = split_data[split_labels == label]\n",
    "            if centroids_per_class[label] > 0 and len(class_data) > 0:\n",
    "                kmeans = KMeans(n_clusters=min(len(class_data), centroids_per_class[label]), random_state=42)\n",
    "                kmeans.fit(class_data)\n",
    "                if label in center_labels:\n",
    "                    center_labels[label].extend(kmeans.cluster_centers_)\n",
    "                else:\n",
    "                    center_labels[label] = list(kmeans.cluster_centers_)\n",
    "\n",
    "        client_iter_end = time.time()\n",
    "        client_times.append(client_iter_end - client_iter_start)\n",
    "\n",
    "    client_end_time = time.time()\n",
    "    client_total_time = client_end_time - client_start_time\n",
    "    client_avg_time = np.mean(client_times)\n",
    "\n",
    "    feature_dim = X.shape[1]\n",
    "    comm_cost = calculate_communication_cost(centroids_per_split, feature_dim)\n",
    "\n",
    "    logging.info(f\"Client training times: Total={client_total_time:.7f}s, Average={client_avg_time:.7f}s\")\n",
    "    logging.info(f\"Per client communication cost: {comm_cost/num_splits:.7f}KB\")\n",
    "\n",
    "    final_centers = [center for sublist in center_labels.values() for center in sublist]\n",
    "    final_labels = [label for label, centers in center_labels.items() for center in centers]\n",
    "\n",
    "    best_model = KNeighborsClassifier(n_neighbors=args.k)\n",
    "    best_model.fit(final_centers, final_labels)\n",
    "\n",
    "    noise_levels = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "    n_bootstrap = 100\n",
    "\n",
    "    noise_results, p_values_acc, p_values_f1 = evaluate_robustness(\n",
    "        best_model, X_test_scaled, y_test, noise_levels, n_bootstrap)\n",
    "\n",
    "    summary_file = os.path.join(RESULTS_DIR, 'experiment_summary.txt')\n",
    "    with open(summary_file, 'w') as f:\n",
    "        f.write(f\"===== EXPERIMENT SUMMARY =====\\n\")\n",
    "        f.write(f\"Experiment Name: {args.experiment_name}\\n\")\n",
    "        f.write(f\"Number of Clients: {args.num_clients}\\n\")\n",
    "        f.write(f\"KNN n_neighbors (k): {args.k}\\n\")\n",
    "        f.write(f\"Max Centroids: {max_centroids}\\n\")\n",
    "        f.write(f\"Optimization Iterations: {args.n_calls}\\n\")\n",
    "        f.write(f\"Total Execution Time: {total_time:.2f} seconds\\n\\n\")\n",
    "\n",
    "        f.write(f\"===== BEST RESULTS =====\\n\")\n",
    "        f.write(f\"Best Centroids: {best_centroids}\\n\")\n",
    "        f.write(f\"Accuracy: {metrics['acc_mean']:.7f} ± {metrics['acc_std']:.7f}\\n\")\n",
    "        f.write(f\"Precision: {metrics['prec_mean']:.7f} ± {metrics['prec_std']:.7f}\\n\")\n",
    "        f.write(f\"Recall: {metrics['rec_mean']:.7f} ± {metrics['rec_std']:.7f}\\n\")\n",
    "        f.write(f\"F1 Score: {metrics['f1_mean']:.7f} ± {metrics['f1_std']:.7f}\\n\")\n",
    "        f.write(f\"Client Time: {metrics['client_time_mean']:.7f}s ± {metrics['client_time_std']:.7f}s\\n\")\n",
    "        f.write(f\"Communication Cost: {metrics['comm_cost_mean']:.7f}KB ± {metrics['comm_cost_std']:.7f}KB\\n\")\n",
    "\n",
    "    logging.info(f\"Process completed. Results were saved to the {RESULTS_DIR} folder.\")\n",
    "    logging.info(f\"Experiment summary: {summary_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "72123eff36e7958c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Fed3C (ABC)",
   "id": "85c3675427e846c2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T08:12:11.091878Z",
     "start_time": "2025-05-18T08:12:11.089716Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import argparse\n",
    "import random  # ABC algoritması için eklendi\n",
    "from scipy.io import arff\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import logging\n",
    "from joblib import Parallel, delayed\n",
    "from scipy import stats\n",
    "\n",
    "args = type('', (), {})()\n",
    "args.num_clients = 10\n",
    "args.max_centroids = None\n",
    "args.n_calls = 50\n",
    "args.n_bees = 10\n",
    "args.k = 1\n",
    "args.experiment_name = \"k1_rice_abc_experiment\"\n",
    "\n",
    "if args.experiment_name is None:\n",
    "    exp_name = f\"clients_{args.num_clients}_abc_{args.n_calls}_bees_{args.n_bees}_k_{args.k}\"\n",
    "    if args.max_centroids is not None:\n",
    "        exp_name += f\"_maxcentroid_{args.max_centroids}\"\n",
    "    args.experiment_name = exp_name\n",
    "\n",
    "RESULTS_ROOT = \"results\"\n",
    "RESULTS_DIR = os.path.join(RESULTS_ROOT, args.experiment_name)\n",
    "\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "log_file_path = os.path.join(RESULTS_DIR, 'rice_abc.log')\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "                    handlers=[logging.FileHandler(log_file_path), logging.StreamHandler()])\n",
    "\n",
    "logging.info(f\"===== EXPERIMENT CONFIGURATION =====\")\n",
    "logging.info(f\"Experiment Name: {args.experiment_name}\")\n",
    "logging.info(f\"Number of Clients: {args.num_clients}\")\n",
    "logging.info(f\"Optimization Algorithm: Artificial Bee Colony (ABC)\")\n",
    "logging.info(f\"Number of Bees: {args.n_bees}\")\n",
    "logging.info(f\"Optimization Max Iterations: {args.n_calls}\")\n",
    "logging.info(f\"KNN n_neighbors (k): {args.k}\")\n",
    "logging.info(f\"Max Centroids: {'Half of training data' if args.max_centroids is None else args.max_centroids}\")\n",
    "logging.info(f\"Results Directory: {RESULTS_DIR}\")\n",
    "\n",
    "data_path = 'rice/Rice_Cammeo_Osmancik.arff'\n",
    "data, meta = arff.loadarff(data_path)\n",
    "data_df = pd.DataFrame(data)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "data_df['Class'] = label_encoder.fit_transform(data_df['Class'].astype(str))\n",
    "\n",
    "X = data_df.drop(columns=['Class'])\n",
    "y = data_df['Class']\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "total_data = len(data_df)\n",
    "train_data_per_fold = total_data * (4/5)\n",
    "\n",
    "if args.max_centroids is not None:\n",
    "    max_centroids = args.max_centroids\n",
    "else:\n",
    "    max_centroids = int(train_data_per_fold / 2)\n",
    "\n",
    "logging.info(f\"Calculated Max Centroids: {max_centroids}\")\n",
    "\n",
    "best_accuracy = float('-inf')\n",
    "best_centroids = None\n",
    "best_accuracies = []\n",
    "best_precisions = []\n",
    "best_recalls = []\n",
    "best_f1s = []\n",
    "best_times = []\n",
    "best_client_times = []\n",
    "best_comm_costs = []\n",
    "best_y_test = None\n",
    "best_predictions = None\n",
    "\n",
    "all_centroids = []\n",
    "all_accuracies = []\n",
    "all_precisions = []\n",
    "all_recalls = []\n",
    "all_f1s = []\n",
    "all_times = []\n",
    "all_client_times = []\n",
    "all_comm_costs = []\n",
    "\n",
    "def log_details(split_data, centroids_per_split):\n",
    "    for i, (data, centroids) in enumerate(zip(split_data, centroids_per_split)):\n",
    "        logging.info(f\"Client {i+1}: Data count = {len(data)}, Number of assigned centroids = {int(centroids)}\")\n",
    "\n",
    "def distribute_centroids(n_data, total_centroids):\n",
    "    proportions = n_data / np.sum(n_data)\n",
    "    floored_centroids = np.floor(proportions * total_centroids).astype(int)\n",
    "    remainder = total_centroids - np.sum(floored_centroids)\n",
    "\n",
    "    while remainder > 0:\n",
    "        idx = np.argmax(proportions - floored_centroids / total_centroids)\n",
    "        floored_centroids[idx] += 1\n",
    "        remainder -= 1\n",
    "\n",
    "    return floored_centroids\n",
    "\n",
    "def calculate_communication_cost(centroids_per_split, feature_dim):\n",
    "    bytes_per_centroid = feature_dim * 8\n",
    "    total_bytes = sum(centroids_per_split) * bytes_per_centroid\n",
    "    return total_bytes / 1024\n",
    "\n",
    "def process_fold(train_index, test_index, total_centroids, X, y):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X.iloc[train_index])\n",
    "    X_test_scaled = scaler.transform(X.iloc[test_index])\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    num_splits = args.num_clients\n",
    "    split_train_data = np.array_split(X_train_scaled, num_splits)\n",
    "    split_train_labels = np.array_split(y_train, num_splits)\n",
    "    data_counts = np.array([len(data) for data in split_train_data])\n",
    "    centroids_per_split = distribute_centroids(data_counts, total_centroids)\n",
    "\n",
    "    center_labels = {}\n",
    "    for split_data, split_labels, centroids_count in zip(split_train_data, split_train_labels, centroids_per_split):\n",
    "        unique_labels, counts = np.unique(split_labels, return_counts=True)\n",
    "        label_counts = dict(zip(unique_labels, counts / counts.sum()))\n",
    "        centroids_per_class = {label: max(1, int(count * centroids_count))\n",
    "                              for label, count in label_counts.items()}\n",
    "\n",
    "        total_class_centroids = sum(centroids_per_class.values())\n",
    "        if total_class_centroids > centroids_count:\n",
    "            scale_factor = centroids_count / total_class_centroids\n",
    "            centroids_per_class = {label: max(1, int(count * scale_factor))\n",
    "                                  for label, count in centroids_per_class.items()}\n",
    "\n",
    "        for label in unique_labels:\n",
    "            class_data = split_data[split_labels == label]\n",
    "            if centroids_per_class[label] > 0 and len(class_data) > 0:\n",
    "                kmeans = KMeans(n_clusters=min(len(class_data), centroids_per_class[label]), random_state=42)\n",
    "                kmeans.fit(class_data)\n",
    "                if label in center_labels:\n",
    "                    center_labels[label].extend(kmeans.cluster_centers_)\n",
    "                else:\n",
    "                    center_labels[label] = list(kmeans.cluster_centers_)\n",
    "\n",
    "    final_centers = [center for sublist in center_labels.values() for center in sublist]\n",
    "    final_labels = [label for label, centers in center_labels.items() for center in centers]\n",
    "\n",
    "    knn = KNeighborsClassifier(n_neighbors=args.k)\n",
    "    knn.fit(final_centers, final_labels)\n",
    "    predictions = knn.predict(X_test_scaled)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    precision = precision_score(y_test, predictions, average='weighted')\n",
    "    recall = recall_score(y_test, predictions, average='weighted')\n",
    "    f1 = f1_score(y_test, predictions, average='weighted')\n",
    "\n",
    "    end_time = time.time()\n",
    "    exec_time = end_time - start_time\n",
    "\n",
    "    feature_dim = X.shape[1]\n",
    "    comm_cost = calculate_communication_cost(centroids_per_split, feature_dim)\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'time': exec_time,\n",
    "        'comm_cost': comm_cost,\n",
    "        'y_test': y_test,\n",
    "        'predictions': predictions\n",
    "    }\n",
    "\n",
    "def objective_sequential(total_centroids):\n",
    "    logging.info(f\"Total number of centroids tried = {total_centroids}\")\n",
    "\n",
    "    fold_accuracies = []\n",
    "    fold_precisions = []\n",
    "    fold_recalls = []\n",
    "    fold_f1s = []\n",
    "    fold_times = []\n",
    "    fold_client_times = []\n",
    "    fold_comm_costs = []\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        fold_start_time = time.time()\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X.iloc[train_index])\n",
    "        X_test_scaled = scaler.transform(X.iloc[test_index])\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        num_splits = args.num_clients\n",
    "        split_train_data = np.array_split(X_train_scaled, num_splits)\n",
    "        split_train_labels = np.array_split(y_train, num_splits)\n",
    "        data_counts = np.array([len(data) for data in split_train_data])\n",
    "        centroids_per_split = distribute_centroids(data_counts, total_centroids)\n",
    "\n",
    "        center_labels = {}\n",
    "        client_times = []\n",
    "\n",
    "        for split_data, split_labels, centroids_count in zip(split_train_data, split_train_labels, centroids_per_split):\n",
    "            client_start_time = time.time()\n",
    "\n",
    "            unique_labels, counts = np.unique(split_labels, return_counts=True)\n",
    "            label_counts = dict(zip(unique_labels, counts / counts.sum()))\n",
    "            centroids_per_class = {label: max(1, int(count * centroids_count))\n",
    "                                  for label, count in label_counts.items()}\n",
    "\n",
    "            total_class_centroids = sum(centroids_per_class.values())\n",
    "            if total_class_centroids > centroids_count:\n",
    "                scale_factor = centroids_count / total_class_centroids\n",
    "                centroids_per_class = {label: max(1, int(count * scale_factor))\n",
    "                                      for label, count in centroids_per_class.items()}\n",
    "\n",
    "            for label in unique_labels:\n",
    "                class_data = split_data[split_labels == label]\n",
    "                if centroids_per_class[label] > 0 and len(class_data) > 0:\n",
    "                    kmeans = KMeans(n_clusters=min(len(class_data), centroids_per_class[label]), random_state=42)\n",
    "                    kmeans.fit(class_data)\n",
    "                    if label in center_labels:\n",
    "                        center_labels[label].extend(kmeans.cluster_centers_)\n",
    "                    else:\n",
    "                        center_labels[label] = list(kmeans.cluster_centers_)\n",
    "\n",
    "            client_end_time = time.time()\n",
    "            client_times.append(client_end_time - client_start_time)\n",
    "\n",
    "        final_centers = [center for sublist in center_labels.values() for center in sublist]\n",
    "        final_labels = [label for label, centers in center_labels.items() for center in centers]\n",
    "\n",
    "        knn = KNeighborsClassifier(n_neighbors=args.k)\n",
    "        knn.fit(final_centers, final_labels)\n",
    "        predictions = knn.predict(X_test_scaled)\n",
    "\n",
    "        accuracy = accuracy_score(y_test, predictions)\n",
    "        precision = precision_score(y_test, predictions, average='weighted')\n",
    "        recall = recall_score(y_test, predictions, average='weighted')\n",
    "        f1 = f1_score(y_test, predictions, average='weighted')\n",
    "\n",
    "        fold_end_time = time.time()\n",
    "        fold_exec_time = fold_end_time - fold_start_time\n",
    "\n",
    "        feature_dim = X.shape[1]\n",
    "        comm_cost = calculate_communication_cost(centroids_per_split, feature_dim)\n",
    "\n",
    "        fold_accuracies.append(accuracy)\n",
    "        fold_precisions.append(precision)\n",
    "        fold_recalls.append(recall)\n",
    "        fold_f1s.append(f1)\n",
    "        fold_times.append(fold_exec_time)\n",
    "        fold_client_times.append(client_times)\n",
    "        fold_comm_costs.append(comm_cost)\n",
    "\n",
    "    mean_accuracy = np.mean(fold_accuracies)\n",
    "    mean_precision = np.mean(fold_precisions)\n",
    "    mean_recall = np.mean(fold_recalls)\n",
    "    mean_f1 = np.mean(fold_f1s)\n",
    "    mean_time = np.mean(fold_times)\n",
    "    mean_client_time = np.mean([np.mean(client_times) for client_times in fold_client_times])\n",
    "    mean_comm_cost = np.mean(fold_comm_costs)\n",
    "\n",
    "    global best_accuracy, best_centroids, best_accuracies, best_precisions, best_recalls, best_f1s, best_times, best_client_times, best_comm_costs, best_y_test, best_predictions\n",
    "\n",
    "    all_centroids.append(total_centroids)\n",
    "    all_accuracies.append(mean_accuracy)\n",
    "    all_precisions.append(mean_precision)\n",
    "    all_recalls.append(mean_recall)\n",
    "    all_f1s.append(mean_f1)\n",
    "    all_times.append(mean_time)\n",
    "    all_client_times.append(mean_client_time)\n",
    "    all_comm_costs.append(mean_comm_cost)\n",
    "\n",
    "    if mean_accuracy > best_accuracy:\n",
    "        best_accuracy = mean_accuracy\n",
    "        best_centroids = total_centroids\n",
    "        best_accuracies = fold_accuracies\n",
    "        best_precisions = fold_precisions\n",
    "        best_recalls = fold_recalls\n",
    "        best_f1s = fold_f1s\n",
    "        best_times = fold_times\n",
    "        best_client_times = [np.mean(client_times) for client_times in fold_client_times]\n",
    "        best_comm_costs = fold_comm_costs\n",
    "\n",
    "        if len(fold_accuracies) > 0:\n",
    "            best_fold_idx = np.argmax(fold_accuracies)\n",
    "            best_fold_indices = list(kf.split(X))[best_fold_idx]\n",
    "            test_indices = best_fold_indices[1]\n",
    "            best_y_test = y.iloc[test_indices]\n",
    "\n",
    "            best_X_test = X.iloc[test_indices]\n",
    "            best_X_test_scaled = scaler.transform(best_X_test)\n",
    "            best_predictions = knn.predict(best_X_test_scaled)\n",
    "\n",
    "    logging.info(f\"Centroids: {total_centroids}, Acc: {mean_accuracy:.7f}, Prec: {mean_precision:.7f}, \" +\n",
    "                 f\"Rec: {mean_recall:.7f}, F1: {mean_f1:.7f}, Time (fold): {mean_time:.2f}s, \" +\n",
    "                 f\"Time (client): {mean_client_time:.7f}s, Comm: {mean_comm_cost:.2f}KB\")\n",
    "\n",
    "    return mean_accuracy, fold_accuracies\n",
    "\n",
    "def abc_algorithm(objective, dimension_bounds, n_bees=30, max_iter=100):\n",
    "    lower_bound, upper_bound = dimension_bounds\n",
    "\n",
    "    population = [random.randint(lower_bound, upper_bound) for _ in range(n_bees)]\n",
    "    fitness_values = []\n",
    "    all_fold_accuracies = []\n",
    "\n",
    "    for centroid in population:\n",
    "        fitness, fold_accuracies = objective(centroid)\n",
    "        fitness_values.append(fitness)\n",
    "        all_fold_accuracies.append(fold_accuracies)\n",
    "\n",
    "    for iter in range(max_iter):\n",
    "        logging.info(f\"ABC Iteration {iter+1}/{max_iter}\")\n",
    "\n",
    "        for i in range(n_bees):\n",
    "            candidate = population[i] + random.randint(-5, 5)\n",
    "            candidate = max(lower_bound, min(candidate, upper_bound))\n",
    "\n",
    "            candidate_fitness, candidate_fold_accuracies = objective(candidate)\n",
    "\n",
    "            if candidate_fitness > fitness_values[i]:\n",
    "                population[i] = candidate\n",
    "                fitness_values[i] = candidate_fitness\n",
    "                all_fold_accuracies[i] = candidate_fold_accuracies\n",
    "\n",
    "    best_index = fitness_values.index(max(fitness_values))\n",
    "    best_centroids = population[best_index]\n",
    "    best_accuracy = fitness_values[best_index]\n",
    "    best_fold_accuracies = all_fold_accuracies[best_index]\n",
    "\n",
    "    return best_centroids, best_accuracy, np.std(best_fold_accuracies)\n",
    "\n",
    "def evaluate_robustness(best_model, X_test, y_test, noise_levels=[0.1, 0.2, 0.3, 0.4, 0.5], n_bootstrap=100):\n",
    "    results = []\n",
    "\n",
    "    n_samples = len(y_test)\n",
    "\n",
    "    original_pred = best_model.predict(X_test)\n",
    "    original_acc = accuracy_score(y_test, original_pred)\n",
    "    original_f1 = f1_score(y_test, original_pred, average='weighted')\n",
    "    results.append(('No Noise', original_acc, original_f1))\n",
    "\n",
    "    bootstrap_original_acc = []\n",
    "    bootstrap_original_f1 = []\n",
    "\n",
    "    for _ in range(n_bootstrap):\n",
    "        indices = np.random.choice(n_samples, n_samples, replace=True)\n",
    "        X_boot = X_test[indices]\n",
    "        y_boot = y_test.iloc[indices] if hasattr(y_test, 'iloc') else y_test[indices]\n",
    "\n",
    "        boot_pred = best_model.predict(X_boot)\n",
    "        bootstrap_original_acc.append(accuracy_score(y_boot, boot_pred))\n",
    "        bootstrap_original_f1.append(f1_score(y_boot, boot_pred, average='weighted'))\n",
    "\n",
    "    p_values_acc = []\n",
    "    p_values_f1 = []\n",
    "    bootstrap_acc_all = []\n",
    "    bootstrap_f1_all = []\n",
    "\n",
    "    for noise in noise_levels:\n",
    "        bootstrap_acc = []\n",
    "        bootstrap_f1 = []\n",
    "\n",
    "        for _ in range(n_bootstrap):\n",
    "            indices = np.random.choice(n_samples, n_samples, replace=True)\n",
    "            X_boot = X_test[indices].copy()\n",
    "            y_boot = y_test.iloc[indices] if hasattr(y_test, 'iloc') else y_test[indices]\n",
    "\n",
    "            noise_matrix = np.random.normal(0, noise, X_boot.shape)\n",
    "            X_boot += noise_matrix\n",
    "\n",
    "            noisy_pred = best_model.predict(X_boot)\n",
    "            noisy_acc = accuracy_score(y_boot, noisy_pred)\n",
    "            noisy_f1 = f1_score(y_boot, noisy_pred, average='weighted')\n",
    "\n",
    "            bootstrap_acc.append(noisy_acc)\n",
    "            bootstrap_f1.append(noisy_f1)\n",
    "\n",
    "        mean_acc = np.mean(bootstrap_acc)\n",
    "        mean_f1 = np.mean(bootstrap_f1)\n",
    "\n",
    "        t_stat_acc, p_val_acc = stats.ttest_rel(bootstrap_original_acc, bootstrap_acc)\n",
    "        t_stat_f1, p_val_f1 = stats.ttest_rel(bootstrap_original_f1, bootstrap_f1)\n",
    "\n",
    "        results.append((f'Noise {noise:.2f}', mean_acc, mean_f1))\n",
    "        p_values_acc.append(p_val_acc)\n",
    "        p_values_f1.append(p_val_f1)\n",
    "        bootstrap_acc_all.append(bootstrap_acc)\n",
    "        bootstrap_f1_all.append(bootstrap_f1)\n",
    "\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    noise_labels = [r[0] for r in results]\n",
    "    acc_values = [r[1] for r in results]\n",
    "    f1_values = [r[2] for r in results]\n",
    "\n",
    "    x = np.arange(len(noise_labels))\n",
    "    width = 0.35\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    bars_acc = plt.bar(x - width/2, acc_values, width, label='Accuracy')\n",
    "    bars_f1 = plt.bar(x + width/2, f1_values, width, label='F1 Score')\n",
    "    plt.xlabel('Noise Level')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Model Robustness to Noise')\n",
    "    plt.xticks(x, noise_labels, rotation=45)\n",
    "    plt.legend()\n",
    "\n",
    "    for i in range(len(p_values_acc)):\n",
    "        if i > 0:\n",
    "            if p_values_acc[i-1] < 0.001:\n",
    "                plt.text(i - width/2, acc_values[i] - 0.02, '***', ha='center')\n",
    "            elif p_values_acc[i-1] < 0.01:\n",
    "                plt.text(i - width/2, acc_values[i] - 0.02, '**', ha='center')\n",
    "            elif p_values_acc[i-1] < 0.05:\n",
    "                plt.text(i - width/2, acc_values[i] - 0.02, '*', ha='center')\n",
    "\n",
    "            if p_values_f1[i-1] < 0.001:\n",
    "                plt.text(i + width/2, f1_values[i] - 0.02, '***', ha='center')\n",
    "            elif p_values_f1[i-1] < 0.01:\n",
    "                plt.text(i + width/2, f1_values[i] - 0.02, '**', ha='center')\n",
    "            elif p_values_f1[i-1] < 0.05:\n",
    "                plt.text(i + width/2, f1_values[i] - 0.02, '*', ha='center')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    bars_acc = plt.bar(x - width/2, acc_values, width, label='Accuracy')\n",
    "    bars_f1 = plt.bar(x + width/2, f1_values, width, label='F1 Score')\n",
    "    plt.xlabel('Noise Level')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Zoomed View (0.85-1.0 range) with p-values')\n",
    "    plt.xticks(x, noise_labels, rotation=45)\n",
    "    plt.ylim(0.85, 1.0)\n",
    "\n",
    "    for i in range(len(p_values_acc)):\n",
    "        if i > 0:\n",
    "            if p_values_acc[i-1] < 0.001:\n",
    "                plt.text(i - width/2, acc_values[i] + 0.005, '***', ha='center', fontsize=10)\n",
    "            elif p_values_acc[i-1] < 0.01:\n",
    "                plt.text(i - width/2, acc_values[i] + 0.005, '**', ha='center', fontsize=10)\n",
    "            elif p_values_acc[i-1] < 0.05:\n",
    "                plt.text(i - width/2, acc_values[i] + 0.005, '*', ha='center', fontsize=10)\n",
    "\n",
    "            if p_values_f1[i-1] < 0.001:\n",
    "                plt.text(i + width/2, f1_values[i] + 0.005, '***', ha='center', fontsize=10)\n",
    "            elif p_values_f1[i-1] < 0.01:\n",
    "                plt.text(i + width/2, f1_values[i] + 0.005, '**', ha='center', fontsize=10)\n",
    "            elif p_values_f1[i-1] < 0.05:\n",
    "                plt.text(i + width/2, f1_values[i] + 0.005, '*', ha='center', fontsize=10)\n",
    "\n",
    "    for i, (acc, f1) in enumerate(zip(acc_values, f1_values)):\n",
    "        plt.text(i - width/2, acc - 0.01, f'{acc:.7f}', ha='center', va='bottom', fontsize=8, rotation=90)\n",
    "        plt.text(i + width/2, f1 - 0.01, f'{f1:.7f}', ha='center', va='bottom', fontsize=8, rotation=90)\n",
    "\n",
    "    plt.figtext(0.5, 0.01, \"* p<0.05, ** p<0.01, *** p<0.001\", ha=\"center\", fontsize=10)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(RESULTS_DIR, 'rice_noise_robustness.png'))\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(14, 8))\n",
    "\n",
    "    plt.subplot(2, 1, 1)\n",
    "    boxplot_data_acc = [bootstrap_original_acc] + bootstrap_acc_all\n",
    "    plt.boxplot(boxplot_data_acc, labels=noise_labels, showfliers=False)\n",
    "    plt.title('Distribution of Accuracy Scores Across Noise Levels')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    boxplot_data_f1 = [bootstrap_original_f1] + bootstrap_f1_all\n",
    "    plt.boxplot(boxplot_data_f1, labels=noise_labels, showfliers=False)\n",
    "    plt.title('Distribution of F1 Scores Across Noise Levels')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(RESULTS_DIR, 'rice_noise_distribution.png'))\n",
    "    plt.close()\n",
    "\n",
    "    logging.info(\"\\n===== NOISE ROBUSTNESS RESULTS WITH P-VALUES =====\")\n",
    "    logging.info(f\"{'Noise Level':<12} {'Accuracy':<15} {'F1 Score':<15} {'p-val (Acc)':<12} {'p-val (F1)':<12}\")\n",
    "\n",
    "    logging.info(f\"{noise_labels[0]:<12} {acc_values[0]:.7f}      {f1_values[0]:.7f}      -            -\")\n",
    "\n",
    "    for i in range(1, len(noise_labels)):\n",
    "        logging.info(f\"{noise_labels[i]:<12} {acc_values[i]:.7f}      {f1_values[i]:.7f}      {p_values_acc[i-1]:.2e}     {p_values_f1[i-1]:.2e}\")\n",
    "\n",
    "    return results, p_values_acc, p_values_f1\n",
    "\n",
    "def analyze_results():\n",
    "    acc_mean, acc_std = np.mean(best_accuracies), np.std(best_accuracies)\n",
    "    prec_mean, prec_std = np.mean(best_precisions), np.std(best_precisions)\n",
    "    rec_mean, rec_std = np.mean(best_recalls), np.std(best_recalls)\n",
    "    f1_mean, f1_std = np.mean(best_f1s), np.std(best_f1s)\n",
    "    time_mean, time_std = np.mean(best_times), np.std(best_times)\n",
    "    client_time_mean, client_time_std = np.mean(best_client_times), np.std(best_client_times)\n",
    "    comm_cost_mean, comm_cost_std = np.mean(best_comm_costs), np.std(best_comm_costs)\n",
    "\n",
    "    logging.info(\"\\n===== BEST CONFIGURATION RESULTS (CLASS-BALANCED CENTROID ALLOCATION) =====\")\n",
    "    logging.info(f\"Best number of centroids: {best_centroids}\")\n",
    "    logging.info(f\"Number of clients: {args.num_clients}\")\n",
    "    logging.info(f\"KNN n_neighbors (k): {args.k}\")\n",
    "    logging.info(f\"Optimization Algorithm: Artificial Bee Colony (ABC)\")\n",
    "    logging.info(f\"Centroid allocation strategy: Class-balanced (Allocation to each class proportional to its data size)\")\n",
    "    logging.info(f\"Accuracy: {acc_mean:.7f} ± {acc_std:.7f}\")\n",
    "    logging.info(f\"Precision: {prec_mean:.7f} ± {prec_std:.7f}\")\n",
    "    logging.info(f\"Recall: {rec_mean:.7f} ± {rec_std:.7f}\")\n",
    "    logging.info(f\"F1 Score: {f1_mean:.7f} ± {f1_std:.7f}\")\n",
    "    logging.info(f\"Average time per fold: {time_mean:.7f}s ± {time_std:.7f}s\")\n",
    "    logging.info(f\"Average time per client: {client_time_mean:.7f}s ± {client_time_std:.7f}s\")\n",
    "    logging.info(f\"Communication cost: {comm_cost_mean:.7f}KB ± {comm_cost_std:.7f}KB\")\n",
    "\n",
    "    if best_y_test is not None:\n",
    "        unique_labels, counts = np.unique(best_y_test, return_counts=True)\n",
    "        class_ratios = dict(zip(unique_labels, counts / counts.sum()))\n",
    "\n",
    "        for label, ratio in class_ratios.items():\n",
    "            class_name = label_encoder.inverse_transform([label])[0] if hasattr(label_encoder, 'inverse_transform') else f\"Class {label}\"\n",
    "            logging.info(f\"Class ratio ({class_name}): {ratio:.7f}\")\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "    values = [acc_mean, prec_mean, rec_mean, f1_mean]\n",
    "    errors = [acc_std, prec_std, rec_std, f1_std]\n",
    "\n",
    "    bars = plt.bar(metrics, values, yerr=errors, capsize=10)\n",
    "    plt.title(f'Performance Metrics with Error Bars (Centroids: {best_centroids}, Clients: {args.num_clients})')\n",
    "    plt.ylabel('Score')\n",
    "    plt.ylim(0, 1.1)\n",
    "\n",
    "    for bar, val, err in zip(bars, values, errors):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., val + err + 0.02,\n",
    "                f'{val:.7f}±{err:.7f}', ha='center', va='bottom', rotation=0)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(RESULTS_DIR, 'rice_performance_metrics.png'))\n",
    "    plt.close()\n",
    "\n",
    "    if best_y_test is not None and best_predictions is not None:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        cm = confusion_matrix(best_y_test, best_predictions)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.title(f'Confusion Matrix (Centroids: {best_centroids}, Clients: {args.num_clients})')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Actual')\n",
    "        plt.savefig(os.path.join(RESULTS_DIR, 'rice_confusion_matrix.png'))\n",
    "        plt.close()\n",
    "\n",
    "    plt.figure(figsize=(14, 8))\n",
    "\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(range(len(all_centroids)), all_accuracies, 'b-', label='Accuracy')\n",
    "    plt.plot(range(len(all_centroids)), all_f1s, 'g-', label='F1 Score')\n",
    "    plt.axvline(x=all_centroids.index(best_centroids), color='r', linestyle='--', label=f'Best Centroids={best_centroids}')\n",
    "    plt.title('Accuracy and F1 Score vs. Iteration')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(range(len(all_centroids)), all_precisions, 'm-', label='Precision')\n",
    "    plt.plot(range(len(all_centroids)), all_recalls, 'c-', label='Recall')\n",
    "    plt.axvline(x=all_centroids.index(best_centroids), color='r', linestyle='--', label=f'Best Centroids={best_centroids}')\n",
    "    plt.title('Precision and Recall vs. Iteration')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(range(len(all_centroids)), all_times, 'k-', label='Fold Time')\n",
    "    plt.plot(range(len(all_centroids)), all_client_times, 'y-', label='Client Time')\n",
    "    plt.axvline(x=all_centroids.index(best_centroids), color='r', linestyle='--', label=f'Best Centroids={best_centroids}')\n",
    "    plt.title('Execution Time vs. Iteration')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Time (s)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(range(len(all_centroids)), all_comm_costs, 'r-', label='Comm Cost')\n",
    "    plt.axvline(x=all_centroids.index(best_centroids), color='b', linestyle='--', label=f'Best Centroids={best_centroids}')\n",
    "    plt.title('Communication Cost vs. Iteration')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Communication Cost (KB)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(RESULTS_DIR, 'rice_optimization_progress.png'))\n",
    "    plt.close()\n",
    "\n",
    "    return {\n",
    "        'acc_mean': acc_mean, 'acc_std': acc_std,\n",
    "        'prec_mean': prec_mean, 'prec_std': prec_std,\n",
    "        'rec_mean': rec_mean, 'rec_std': rec_std,\n",
    "        'f1_mean': f1_mean, 'f1_std': f1_std,\n",
    "        'time_mean': time_mean, 'time_std': time_std,\n",
    "        'client_time_mean': client_time_mean, 'client_time_std': client_time_std,\n",
    "        'comm_cost_mean': comm_cost_mean, 'comm_cost_std': comm_cost_std\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    start_time = time.time()\n",
    "\n",
    "    logging.info(f\"Artificial Bee Colony (ABC) optimization starting (with class-balanced centroid allocation)...\")\n",
    "    logging.info(f\"Parameter configuration: Number of Clients={args.num_clients}, Max Centroid={max_centroids}, Number of Bees={args.n_bees}, Maximum Iterations={args.n_calls}\")\n",
    "\n",
    "    dimension_bounds = (10, max_centroids)\n",
    "\n",
    "    best_centroids_abc, best_accuracy_abc, std_dev_abc = abc_algorithm(\n",
    "        objective_sequential,\n",
    "        dimension_bounds,\n",
    "        n_bees=args.n_bees,\n",
    "        max_iter=args.n_calls\n",
    "    )\n",
    "\n",
    "    global best_centroids\n",
    "    best_centroids = best_centroids_abc\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    logging.info(f\"Optimization completed. Total time: {total_time:.7f} seconds\")\n",
    "    logging.info(f\"Best number of centroids: {best_centroids}, Accuracy: {best_accuracy:.7f}, Standard Deviation: {std_dev_abc:.7f}\")\n",
    "\n",
    "    metrics = analyze_results()\n",
    "\n",
    "    logging.info(\"Running noise robustness test...\")\n",
    "\n",
    "    best_fold_idx = np.argmax(best_accuracies)\n",
    "    train_indices = list(kf.split(X))[best_fold_idx][0]\n",
    "    test_indices = list(kf.split(X))[best_fold_idx][1]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X.iloc[train_indices])\n",
    "    X_test_scaled = scaler.transform(X.iloc[test_indices])\n",
    "    y_train, y_test = y.iloc[train_indices], y.iloc[test_indices]\n",
    "\n",
    "    client_start_time = time.time()\n",
    "\n",
    "    num_splits = args.num_clients\n",
    "    split_train_data = np.array_split(X_train_scaled, num_splits)\n",
    "    split_train_labels = np.array_split(y_train, num_splits)\n",
    "    data_counts = np.array([len(data) for data in split_train_data])\n",
    "    centroids_per_split = distribute_centroids(data_counts, best_centroids)\n",
    "\n",
    "    client_times = []\n",
    "\n",
    "    center_labels = {}\n",
    "    for split_data, split_labels, centroids_count in zip(split_train_data, split_train_labels, centroids_per_split):\n",
    "        client_iter_start = time.time()\n",
    "\n",
    "        unique_labels, counts = np.unique(split_labels, return_counts=True)\n",
    "        label_counts = dict(zip(unique_labels, counts / counts.sum()))\n",
    "        centroids_per_class = {label: max(1, int(count * centroids_count))\n",
    "                              for label, count in label_counts.items()}\n",
    "\n",
    "        total_class_centroids = sum(centroids_per_class.values())\n",
    "        if total_class_centroids > centroids_count:\n",
    "            scale_factor = centroids_count / total_class_centroids\n",
    "            centroids_per_class = {label: max(1, int(count * scale_factor))\n",
    "                                   for label, count in centroids_per_class.items()}\n",
    "\n",
    "        logging.info(f\"Centroid distribution: {centroids_per_class}\")\n",
    "\n",
    "        for label in unique_labels:\n",
    "            class_data = split_data[split_labels == label]\n",
    "            if centroids_per_class[label] > 0 and len(class_data) > 0:\n",
    "                kmeans = KMeans(n_clusters=min(len(class_data), centroids_per_class[label]), random_state=42)\n",
    "                kmeans.fit(class_data)\n",
    "                if label in center_labels:\n",
    "                    center_labels[label].extend(kmeans.cluster_centers_)\n",
    "                else:\n",
    "                    center_labels[label] = list(kmeans.cluster_centers_)\n",
    "\n",
    "        client_iter_end = time.time()\n",
    "        client_times.append(client_iter_end - client_iter_start)\n",
    "\n",
    "    client_end_time = time.time()\n",
    "    client_total_time = client_end_time - client_start_time\n",
    "    client_avg_time = np.mean(client_times)\n",
    "\n",
    "    feature_dim = X.shape[1]\n",
    "    comm_cost = calculate_communication_cost(centroids_per_split, feature_dim)\n",
    "\n",
    "    logging.info(f\"Client training times: Total={client_total_time:.7f}s, Average={client_avg_time:.7f}s\")\n",
    "    logging.info(f\"Per client communication cost: {comm_cost/num_splits:.7f}KB\")\n",
    "\n",
    "    final_centers = [center for sublist in center_labels.values() for center in sublist]\n",
    "    final_labels = [label for label, centers in center_labels.items() for center in centers]\n",
    "\n",
    "    best_model = KNeighborsClassifier(n_neighbors=args.k)\n",
    "    best_model.fit(final_centers, final_labels)\n",
    "\n",
    "    noise_levels = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "    n_bootstrap = 100  # Bootstrap örnek sayısı\n",
    "\n",
    "    noise_results, p_values_acc, p_values_f1 = evaluate_robustness(\n",
    "        best_model, X_test_scaled, y_test, noise_levels, n_bootstrap)\n",
    "\n",
    "    summary_file = os.path.join(RESULTS_DIR, 'experiment_summary.txt')\n",
    "    with open(summary_file, 'w') as f:\n",
    "        f.write(f\"===== EXPERIMENT SUMMARY =====\\n\")\n",
    "        f.write(f\"Experiment Name: {args.experiment_name}\\n\")\n",
    "        f.write(f\"Number of Clients: {args.num_clients}\\n\")\n",
    "        f.write(f\"Optimization Algorithm: Artificial Bee Colony (ABC)\\n\")\n",
    "        f.write(f\"Number of Bees: {args.n_bees}\\n\")\n",
    "        f.write(f\"Max Iterations: {args.n_calls}\\n\")\n",
    "        f.write(f\"KNN n_neighbors (k): {args.k}\\n\")\n",
    "        f.write(f\"Max Centroids: {max_centroids}\\n\")\n",
    "        f.write(f\"Total Execution Time: {total_time:.2f} seconds\\n\\n\")\n",
    "\n",
    "        f.write(f\"===== BEST RESULTS =====\\n\")\n",
    "        f.write(f\"Best Centroids: {best_centroids}\\n\")\n",
    "        f.write(f\"Accuracy: {metrics['acc_mean']:.7f} ± {metrics['acc_std']:.7f}\\n\")\n",
    "        f.write(f\"Precision: {metrics['prec_mean']:.7f} ± {metrics['prec_std']:.7f}\\n\")\n",
    "        f.write(f\"Recall: {metrics['rec_mean']:.7f} ± {metrics['rec_std']:.7f}\\n\")\n",
    "        f.write(f\"F1 Score: {metrics['f1_mean']:.7f} ± {metrics['f1_std']:.7f}\\n\")\n",
    "        f.write(f\"Client Time: {metrics['client_time_mean']:.7f}s ± {metrics['client_time_std']:.7f}s\\n\")\n",
    "        f.write(f\"Communication Cost: {metrics['comm_cost_mean']:.7f}KB ± {metrics['comm_cost_std']:.7f}KB\\n\")\n",
    "\n",
    "    logging.info(f\"Process completed. Results were saved to the {RESULTS_DIR} folder.\")\n",
    "    logging.info(f\"Experiment summary: {summary_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "a9e81b887470d2b9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## FlyNNFL",
   "id": "c863a94057686886"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T08:12:07.458988Z",
     "start_time": "2025-05-18T08:12:07.456226Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Integer\n",
    "from skopt.utils import use_named_args\n",
    "from scipy.io import arff\n",
    "import logging\n",
    "from joblib import Parallel, delayed\n",
    "from scipy import stats\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "                    handlers=[logging.FileHandler('rice_flynn.log'), logging.StreamHandler()])\n",
    "\n",
    "\n",
    "class FlyNN:\n",
    "    def __init__(self, m=2000, s=10, rho=20, gamma=0.5, random_state=42):\n",
    "\n",
    "        self.m = m\n",
    "        self.s = s\n",
    "        self.rho = rho\n",
    "        self.gamma = gamma\n",
    "        self.random_state = random_state\n",
    "        self.M = None\n",
    "        self.class_fbf = {}\n",
    "\n",
    "    def _generate_lifting_matrix(self, d):\n",
    "        np.random.seed(self.random_state)\n",
    "        M = np.zeros((self.m, d), dtype=np.bool_)\n",
    "\n",
    "        actual_s = min(self.s, d)\n",
    "\n",
    "        for i in range(self.m):\n",
    "            nonzero_indices = np.random.choice(d, actual_s, replace=False)\n",
    "            M[i, nonzero_indices] = True\n",
    "\n",
    "        return M\n",
    "\n",
    "    def _flyhash(self, x):\n",
    "        if self.M is None:\n",
    "            raise ValueError(\"Lifting matrix M has not been initialized\")\n",
    "\n",
    "        if hasattr(x, 'values'):\n",
    "            x = x.values\n",
    "\n",
    "        projections = self.M.dot(x)\n",
    "\n",
    "        actual_rho = min(self.rho, self.m)\n",
    "\n",
    "        h = np.zeros(self.m, dtype=np.bool_)\n",
    "        top_indices = np.argsort(projections)[-actual_rho:]\n",
    "        h[top_indices] = True\n",
    "\n",
    "        return h\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        d = X.shape[1]\n",
    "        self.M = self._generate_lifting_matrix(d)\n",
    "\n",
    "        unique_classes = np.unique(y)\n",
    "\n",
    "        for cls in unique_classes:\n",
    "            self.class_fbf[cls] = np.ones(self.m, dtype=np.float32)\n",
    "\n",
    "        for i in range(len(X)):\n",
    "            if isinstance(X, (pd.DataFrame, pd.Series)):\n",
    "                x = X.iloc[i]\n",
    "            else:\n",
    "                x = X[i]\n",
    "\n",
    "            if isinstance(y, (pd.DataFrame, pd.Series)):\n",
    "                label = y.iloc[i]\n",
    "            else:\n",
    "                label = y[i]\n",
    "\n",
    "            h = self._flyhash(x)\n",
    "\n",
    "            self.class_fbf[label][h] *= self.gamma\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        if not self.class_fbf:\n",
    "            raise ValueError(\"Model has not been trained\")\n",
    "\n",
    "        predictions = np.zeros(len(X), dtype=int)\n",
    "\n",
    "        for i in range(len(X)):\n",
    "            if isinstance(X, (pd.DataFrame, pd.Series)):\n",
    "                x = X.iloc[i]\n",
    "            else:\n",
    "                x = X[i]\n",
    "\n",
    "            h = self._flyhash(x)\n",
    "\n",
    "            scores = {}\n",
    "            for cls, fbf in self.class_fbf.items():\n",
    "                scores[cls] = np.sum(fbf[h])\n",
    "\n",
    "            predictions[i] = min(scores, key=scores.get)\n",
    "\n",
    "        return predictions\n",
    "\n",
    "def train_client_flynn(X_train, y_train, m, s, rho, gamma, random_state):\n",
    "    client_model = FlyNN(m=m, s=s, rho=rho, gamma=gamma, random_state=random_state)\n",
    "    client_model.fit(X_train, y_train)\n",
    "    return client_model.M, client_model.class_fbf\n",
    "\n",
    "def aggregate_flynn_models(client_models, gamma):\n",
    "    M = client_models[0][0]\n",
    "\n",
    "    all_classes = set()\n",
    "    for _, class_fbf in client_models:\n",
    "        all_classes.update(class_fbf.keys())\n",
    "\n",
    "    aggregated_fbf = {}\n",
    "    for cls in all_classes:\n",
    "        log_sum = 0\n",
    "        count = 0\n",
    "\n",
    "        for _, class_fbf in client_models:\n",
    "            if cls in class_fbf:\n",
    "                log_sum += np.log(class_fbf[cls]) / np.log(gamma)\n",
    "                count += 1\n",
    "\n",
    "        if count > 0:\n",
    "            aggregated_fbf[cls] = np.power(gamma, log_sum / count)\n",
    "\n",
    "    return M, aggregated_fbf\n",
    "\n",
    "def apply_differential_privacy(class_fbf, epsilon, samples, gamma):\n",
    "    log_fbf = {}\n",
    "    for cls, fbf in class_fbf.items():\n",
    "        log_fbf[cls] = np.log(fbf) / np.log(gamma)\n",
    "\n",
    "    dp_log_fbf = {}\n",
    "    for cls, log_counts in log_fbf.items():\n",
    "        dp_log_fbf[cls] = np.zeros_like(log_counts)\n",
    "\n",
    "        probabilities = np.exp(log_counts / (4 * samples))\n",
    "        probabilities = probabilities / np.sum(probabilities)\n",
    "\n",
    "        selected_indices = np.random.choice(\n",
    "            len(log_counts),\n",
    "            size=min(samples, len(log_counts)),\n",
    "            replace=False,\n",
    "            p=probabilities\n",
    "        )\n",
    "\n",
    "        noise = np.random.laplace(0, 2 * samples / epsilon, size=len(selected_indices))\n",
    "        for i, idx in enumerate(selected_indices):\n",
    "            dp_log_fbf[cls][idx] = max(0, log_counts[idx] + noise[i])\n",
    "\n",
    "    dp_fbf = {}\n",
    "    for cls, dp_log_counts in dp_log_fbf.items():\n",
    "        dp_fbf[cls] = np.power(gamma, dp_log_counts)\n",
    "\n",
    "    return dp_fbf\n",
    "\n",
    "def federated_train_flynn(split_data, split_labels, m, s, rho, gamma, random_state, is_dp=False, epsilon=1.0, dp_samples=100):\n",
    "    client_models = []\n",
    "\n",
    "    for client_data, client_labels in zip(split_data, split_labels):\n",
    "        M, class_fbf = train_client_flynn(client_data, client_labels, m, s, rho, gamma, random_state)\n",
    "\n",
    "        if is_dp:\n",
    "            class_fbf = apply_differential_privacy(class_fbf, epsilon, dp_samples, gamma)\n",
    "\n",
    "        client_models.append((M, class_fbf))\n",
    "\n",
    "    M, aggregated_fbf = aggregate_flynn_models(client_models, gamma)\n",
    "\n",
    "    model = FlyNN(m=m, s=s, rho=rho, gamma=gamma, random_state=random_state)\n",
    "    model.M = M\n",
    "    model.class_fbf = aggregated_fbf\n",
    "\n",
    "    return model\n",
    "\n",
    "def evaluate_flynn_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "    return accuracy, precision, recall, f1, y_test, y_pred\n",
    "\n",
    "def calculate_communication_cost(m, num_classes, feature_dim, clients):\n",
    "    bytes_per_fbf = m * 4\n",
    "    bytes_per_client = num_classes * bytes_per_fbf\n",
    "    total_bytes = clients * bytes_per_client\n",
    "\n",
    "    return total_bytes / 1024\n",
    "\n",
    "space = [\n",
    "    Integer(2000, 10000, name='m'),\n",
    "    Integer(5, 50, name='s'),\n",
    "    Integer(10, 100, name='rho'),\n",
    "]\n",
    "\n",
    "@use_named_args(space)\n",
    "def objective_flynn(m, s, rho):\n",
    "    logging.info(f\"Evaluating FlyNN hyperparameters: m={m}, s={s}, rho={rho}\")\n",
    "\n",
    "    gamma = 0.5\n",
    "    num_splits = 10\n",
    "\n",
    "    fold_accuracies = []\n",
    "    fold_precisions = []\n",
    "    fold_recalls = []\n",
    "    fold_f1s = []\n",
    "    fold_times = []\n",
    "    fold_client_times = []\n",
    "    fold_comm_costs = []\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        fold_start_time = time.time()\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X.iloc[train_index])\n",
    "        X_test_scaled = scaler.transform(X.iloc[test_index])\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        X_train_scaled = np.array(X_train_scaled)\n",
    "        X_test_scaled = np.array(X_test_scaled)\n",
    "        y_train = np.array(y_train)\n",
    "        y_test = np.array(y_test)\n",
    "\n",
    "        split_train_data = np.array_split(X_train_scaled, num_splits)\n",
    "        split_train_labels = np.array_split(y_train, num_splits)\n",
    "\n",
    "        client_times = []\n",
    "\n",
    "        client_start_time = time.time()\n",
    "\n",
    "        flynn_model = federated_train_flynn(\n",
    "            split_train_data,\n",
    "            split_train_labels,\n",
    "            m, s, rho, gamma,\n",
    "            random_state=42,\n",
    "            is_dp=False\n",
    "        )\n",
    "\n",
    "        client_end_time = time.time()\n",
    "        client_times.append(client_end_time - client_start_time)\n",
    "\n",
    "        accuracy, precision, recall, f1, _, _ = evaluate_flynn_model(flynn_model, X_test_scaled, y_test)\n",
    "\n",
    "        fold_end_time = time.time()\n",
    "        fold_exec_time = fold_end_time - fold_start_time\n",
    "\n",
    "        num_classes = len(np.unique(y_train))\n",
    "        feature_dim = X.shape[1]\n",
    "        comm_cost = calculate_communication_cost(m, num_classes, feature_dim, num_splits)\n",
    "\n",
    "        fold_accuracies.append(accuracy)\n",
    "        fold_precisions.append(precision)\n",
    "        fold_recalls.append(recall)\n",
    "        fold_f1s.append(f1)\n",
    "        fold_times.append(fold_exec_time)\n",
    "        fold_client_times.append(np.mean(client_times))\n",
    "        fold_comm_costs.append(comm_cost)\n",
    "\n",
    "    mean_accuracy = np.mean(fold_accuracies)\n",
    "    mean_precision = np.mean(fold_precisions)\n",
    "    mean_recall = np.mean(fold_recalls)\n",
    "    mean_f1 = np.mean(fold_f1s)\n",
    "    mean_time = np.mean(fold_times)\n",
    "    mean_client_time = np.mean(fold_client_times)\n",
    "    mean_comm_cost = np.mean(fold_comm_costs)\n",
    "\n",
    "    all_hyperparams.append((m, s, rho))\n",
    "    all_accuracies.append(mean_accuracy)\n",
    "    all_precisions.append(mean_precision)\n",
    "    all_recalls.append(mean_recall)\n",
    "    all_f1s.append(mean_f1)\n",
    "    all_times.append(mean_time)\n",
    "    all_client_times.append(mean_client_time)\n",
    "    all_comm_costs.append(mean_comm_cost)\n",
    "\n",
    "    global best_accuracy, best_params, best_accuracies, best_precisions, best_recalls\n",
    "    global best_f1s, best_times, best_client_times, best_comm_costs, best_y_test, best_predictions\n",
    "\n",
    "    if mean_accuracy > best_accuracy:\n",
    "        best_accuracy = mean_accuracy\n",
    "        best_params = (m, s, rho)\n",
    "        best_accuracies = fold_accuracies\n",
    "        best_precisions = fold_precisions\n",
    "        best_recalls = fold_recalls\n",
    "        best_f1s = fold_f1s\n",
    "        best_times = fold_times\n",
    "        best_client_times = fold_client_times\n",
    "        best_comm_costs = fold_comm_costs\n",
    "\n",
    "    logging.info(f\"FlyNN: m={m}, s={s}, rho={rho}, Acc: {mean_accuracy:.7f}, Prec: {mean_precision:.7f}, \" +\n",
    "                f\"Rec: {mean_recall:.7f}, F1: {mean_f1:.7f}, Time: {mean_time:.2f}s, \" +\n",
    "                f\"Client Time: {mean_client_time:.7f}s, Comm: {mean_comm_cost:.2f}KB\")\n",
    "\n",
    "    return -mean_accuracy\n",
    "\n",
    "\n",
    "def evaluate_flynn_robustness(best_model, X_test, y_test, noise_levels=[0.1, 0.2, 0.3, 0.4, 0.5], n_bootstrap=100):\n",
    "    results = []\n",
    "\n",
    "    X_test = np.array(X_test)\n",
    "    y_test = np.array(y_test)\n",
    "\n",
    "    n_samples = len(y_test)\n",
    "\n",
    "    original_pred = best_model.predict(X_test)\n",
    "    original_acc = accuracy_score(y_test, original_pred)\n",
    "    original_f1 = f1_score(y_test, original_pred, average='weighted')\n",
    "    results.append(('No Noise', original_acc, original_f1))\n",
    "\n",
    "    bootstrap_original_acc = []\n",
    "    bootstrap_original_f1 = []\n",
    "\n",
    "    for _ in range(n_bootstrap):\n",
    "        indices = np.random.choice(n_samples, n_samples, replace=True)\n",
    "        X_boot = X_test[indices]\n",
    "        y_boot = y_test[indices]\n",
    "\n",
    "        boot_pred = best_model.predict(X_boot)\n",
    "        bootstrap_original_acc.append(accuracy_score(y_boot, boot_pred))\n",
    "        bootstrap_original_f1.append(f1_score(y_boot, boot_pred, average='weighted'))\n",
    "\n",
    "    p_values_acc = []\n",
    "    p_values_f1 = []\n",
    "    bootstrap_acc_all = []\n",
    "    bootstrap_f1_all = []\n",
    "\n",
    "    for noise in noise_levels:\n",
    "        bootstrap_acc = []\n",
    "        bootstrap_f1 = []\n",
    "\n",
    "        for _ in range(n_bootstrap):\n",
    "            indices = np.random.choice(n_samples, n_samples, replace=True)\n",
    "            X_boot = X_test[indices].copy()\n",
    "            y_boot = y_test[indices]\n",
    "\n",
    "            noise_matrix = np.random.normal(0, noise, X_boot.shape)\n",
    "            X_boot += noise_matrix\n",
    "\n",
    "            noisy_pred = best_model.predict(X_boot)\n",
    "            noisy_acc = accuracy_score(y_boot, noisy_pred)\n",
    "            noisy_f1 = f1_score(y_boot, noisy_pred, average='weighted')\n",
    "\n",
    "            bootstrap_acc.append(noisy_acc)\n",
    "            bootstrap_f1.append(noisy_f1)\n",
    "\n",
    "        mean_acc = np.mean(bootstrap_acc)\n",
    "        mean_f1 = np.mean(bootstrap_f1)\n",
    "\n",
    "        t_stat_acc, p_val_acc = stats.ttest_rel(bootstrap_original_acc, bootstrap_acc)\n",
    "        t_stat_f1, p_val_f1 = stats.ttest_rel(bootstrap_original_f1, bootstrap_f1)\n",
    "\n",
    "        results.append((f'Noise {noise:.2f}', mean_acc, mean_f1))\n",
    "        p_values_acc.append(p_val_acc)\n",
    "        p_values_f1.append(p_val_f1)\n",
    "        bootstrap_acc_all.append(bootstrap_acc)\n",
    "        bootstrap_f1_all.append(bootstrap_f1)\n",
    "\n",
    "    visualize_robustness_results(results, p_values_acc, p_values_f1, bootstrap_original_acc, bootstrap_original_f1, bootstrap_acc_all, bootstrap_f1_all)\n",
    "\n",
    "    return results, p_values_acc, p_values_f1\n",
    "\n",
    "def visualize_robustness_results(results, p_values_acc, p_values_f1, bootstrap_original_acc, bootstrap_original_f1, bootstrap_acc_all, bootstrap_f1_all):\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    noise_labels = [r[0] for r in results]\n",
    "    acc_values = [r[1] for r in results]\n",
    "    f1_values = [r[2] for r in results]\n",
    "\n",
    "    x = np.arange(len(noise_labels))\n",
    "    width = 0.35\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    bars_acc = plt.bar(x - width/2, acc_values, width, label='Accuracy')\n",
    "    bars_f1 = plt.bar(x + width/2, f1_values, width, label='F1 Score')\n",
    "    plt.xlabel('Noise Level')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('FlyNN Model Robustness to Noise')\n",
    "    plt.xticks(x, noise_labels, rotation=45)\n",
    "    plt.legend()\n",
    "\n",
    "    for i in range(len(p_values_acc)):\n",
    "        if i > 0:\n",
    "            if p_values_acc[i-1] < 0.001:\n",
    "                plt.text(i - width/2, acc_values[i] - 0.02, '***', ha='center')\n",
    "            elif p_values_acc[i-1] < 0.01:\n",
    "                plt.text(i - width/2, acc_values[i] - 0.02, '**', ha='center')\n",
    "            elif p_values_acc[i-1] < 0.05:\n",
    "                plt.text(i - width/2, acc_values[i] - 0.02, '*', ha='center')\n",
    "\n",
    "            if p_values_f1[i-1] < 0.001:\n",
    "                plt.text(i + width/2, f1_values[i] - 0.02, '***', ha='center')\n",
    "            elif p_values_f1[i-1] < 0.01:\n",
    "                plt.text(i + width/2, f1_values[i] - 0.02, '**', ha='center')\n",
    "            elif p_values_f1[i-1] < 0.05:\n",
    "                plt.text(i + width/2, f1_values[i] - 0.02, '*', ha='center')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    bars_acc = plt.bar(x - width/2, acc_values, width, label='Accuracy')\n",
    "    bars_f1 = plt.bar(x + width/2, f1_values, width, label='F1 Score')\n",
    "    plt.xlabel('Noise Level')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Zoomed View (0.85-1.0 range) with p-values')\n",
    "    plt.xticks(x, noise_labels, rotation=45)\n",
    "    plt.ylim(0.85, 1.0)\n",
    "\n",
    "    for i in range(len(p_values_acc)):\n",
    "        if i > 0:\n",
    "            if p_values_acc[i-1] < 0.001:\n",
    "                plt.text(i - width/2, acc_values[i] + 0.005, '***', ha='center', fontsize=10)\n",
    "            elif p_values_acc[i-1] < 0.01:\n",
    "                plt.text(i - width/2, acc_values[i] + 0.005, '**', ha='center', fontsize=10)\n",
    "            elif p_values_acc[i-1] < 0.05:\n",
    "                plt.text(i - width/2, acc_values[i] + 0.005, '*', ha='center', fontsize=10)\n",
    "\n",
    "            if p_values_f1[i-1] < 0.001:\n",
    "                plt.text(i + width/2, f1_values[i] + 0.005, '***', ha='center', fontsize=10)\n",
    "            elif p_values_f1[i-1] < 0.01:\n",
    "                plt.text(i + width/2, f1_values[i] + 0.005, '**', ha='center', fontsize=10)\n",
    "            elif p_values_f1[i-1] < 0.05:\n",
    "                plt.text(i + width/2, f1_values[i] + 0.005, '*', ha='center', fontsize=10)\n",
    "\n",
    "    for i, (acc, f1) in enumerate(zip(acc_values, f1_values)):\n",
    "        plt.text(i - width/2, acc - 0.01, f'{acc:.7f}', ha='center', va='bottom', fontsize=8, rotation=90)\n",
    "        plt.text(i + width/2, f1 - 0.01, f'{f1:.7f}', ha='center', va='bottom', fontsize=8, rotation=90)\n",
    "\n",
    "    plt.figtext(0.5, 0.01, \"* p<0.05, ** p<0.01, *** p<0.001\", ha=\"center\", fontsize=10)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('flynn_noise_robustness.png')\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(14, 8))\n",
    "\n",
    "    plt.subplot(2, 1, 1)\n",
    "    boxplot_data_acc = [bootstrap_original_acc] + bootstrap_acc_all\n",
    "    plt.boxplot(boxplot_data_acc, labels=noise_labels, showfliers=False)\n",
    "    plt.title('Distribution of Accuracy Scores Across Noise Levels (FlyNN)')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    boxplot_data_f1 = [bootstrap_original_f1] + bootstrap_f1_all\n",
    "    plt.boxplot(boxplot_data_f1, labels=noise_labels, showfliers=False)\n",
    "    plt.title('Distribution of F1 Scores Across Noise Levels (FlyNN)')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('flynn_noise_distribution.png')\n",
    "    plt.close()\n",
    "\n",
    "def analyze_flynn_results():\n",
    "    acc_mean, acc_std = np.mean(best_accuracies), np.std(best_accuracies)\n",
    "    prec_mean, prec_std = np.mean(best_precisions), np.std(best_precisions)\n",
    "    rec_mean, rec_std = np.mean(best_recalls), np.std(best_recalls)\n",
    "    f1_mean, f1_std = np.mean(best_f1s), np.std(best_f1s)\n",
    "    time_mean, time_std = np.mean(best_times), np.std(best_times)\n",
    "    client_time_mean, client_time_std = np.mean(best_client_times), np.std(best_client_times)\n",
    "    comm_cost_mean, comm_cost_std = np.mean(best_comm_costs), np.std(best_comm_costs)\n",
    "\n",
    "    logging.info(\"\\n===== FLYNN: BEST CONFIGURATION RESULTS =====\")\n",
    "    logging.info(f\"Best parameters: m={best_params[0]}, s={best_params[1]}, rho={best_params[2]}\")\n",
    "    logging.info(f\"Accuracy: {acc_mean:.7f} ± {acc_std:.7f}\")\n",
    "    logging.info(f\"Precision: {prec_mean:.7f} ± {prec_std:.7f}\")\n",
    "    logging.info(f\"Recall: {rec_mean:.7f} ± {rec_std:.7f}\")\n",
    "    logging.info(f\"F1 Score: {f1_mean:.7f} ± {f1_std:.7f}\")\n",
    "    logging.info(f\"Average time per fold: {time_mean:.7f}s ± {time_std:.7f}s\")\n",
    "    logging.info(f\"Average time per client: {client_time_mean:.7f}s ± {client_time_std:.7f}s\")\n",
    "    logging.info(f\"Communication cost: {comm_cost_mean:.2f}KB ± {comm_cost_std:.2f}KB\")\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "    values = [acc_mean, prec_mean, rec_mean, f1_mean]\n",
    "    errors = [acc_std, prec_std, rec_std, f1_std]\n",
    "\n",
    "    bars = plt.bar(metrics, values, yerr=errors, capsize=10)\n",
    "    plt.title(f'FlyNN Performance Metrics (m={best_params[0]}, s={best_params[1]}, rho={best_params[2]})')\n",
    "    plt.ylabel('Score')\n",
    "    plt.ylim(0, 1.1)\n",
    "\n",
    "    for bar, val, err in zip(bars, values, errors):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., val + err + 0.02,\n",
    "                f'{val:.7f}±{err:.7f}', ha='center', va='bottom', rotation=0)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('flynn_performance_metrics.png')\n",
    "    plt.close()\n",
    "\n",
    "    if best_y_test is not None and best_predictions is not None:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        cm = confusion_matrix(best_y_test, best_predictions)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.title('Confusion Matrix for Best FlyNN Configuration')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Actual')\n",
    "        plt.savefig('flynn_confusion_matrix.png')\n",
    "        plt.close()\n",
    "\n",
    "def main_flynn():\n",
    "    global X, y, kf\n",
    "    global best_accuracy, best_params, best_accuracies, best_precisions, best_recalls\n",
    "    global best_f1s, best_times, best_client_times, best_comm_costs, best_y_test, best_predictions\n",
    "    global all_hyperparams, all_accuracies, all_precisions, all_recalls, all_f1s, all_times, all_client_times, all_comm_costs\n",
    "\n",
    "    data_path = 'rice/Rice_Cammeo_Osmancik.arff'\n",
    "    data, meta = arff.loadarff(data_path)\n",
    "    data_df = pd.DataFrame(data)\n",
    "\n",
    "    data_df['Class'] = data_df['Class'].apply(lambda x: x.decode('utf-8'))\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    data_df['Class'] = label_encoder.fit_transform(data_df['Class'])\n",
    "\n",
    "    X = data_df.drop(columns=['Class'])\n",
    "    y = data_df['Class']\n",
    "\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    all_hyperparams = []\n",
    "    all_accuracies = []\n",
    "    all_precisions = []\n",
    "    all_recalls = []\n",
    "    all_f1s = []\n",
    "    all_times = []\n",
    "    all_client_times = []\n",
    "    all_comm_costs = []\n",
    "\n",
    "    best_accuracy = 0\n",
    "    best_params = None\n",
    "    best_accuracies = []\n",
    "    best_precisions = []\n",
    "    best_recalls = []\n",
    "    best_f1s = []\n",
    "    best_times = []\n",
    "    best_client_times = []\n",
    "    best_comm_costs = []\n",
    "    best_y_test = None\n",
    "    best_predictions = None\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    logging.info(\"FlyNN Bayesian optimization starting...\")\n",
    "    res_gp = gp_minimize(objective_flynn, space, n_calls=50, random_state=0)\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    logging.info(f\"FlyNN optimization completed. Total time: {total_time:.2f} seconds\")\n",
    "\n",
    "    m_best, s_best, rho_best = best_params\n",
    "    logging.info(f\"Best FlyNN parameters: m={m_best}, s={s_best}, rho={rho_best}\")\n",
    "\n",
    "    analyze_flynn_results()\n",
    "\n",
    "    logging.info(\"FlyNN: Running noise robustness test...\")\n",
    "\n",
    "    best_fold_idx = np.argmax(best_accuracies)\n",
    "    train_indices = list(kf.split(X))[best_fold_idx][0]\n",
    "    test_indices = list(kf.split(X))[best_fold_idx][1]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X.iloc[train_indices])\n",
    "    X_test_scaled = scaler.transform(X.iloc[test_indices])\n",
    "    y_train, y_test = y[train_indices], y[test_indices]\n",
    "\n",
    "    X_train_scaled = np.array(X_train_scaled)\n",
    "    X_test_scaled = np.array(X_test_scaled)\n",
    "    y_train = np.array(y_train)\n",
    "    y_test = np.array(y_test)\n",
    "\n",
    "    num_splits = 10\n",
    "    split_train_data = np.array_split(X_train_scaled, num_splits)\n",
    "    split_train_labels = np.array_split(y_train, num_splits)\n",
    "\n",
    "    gamma = 0.5\n",
    "    best_flynn_model = federated_train_flynn(\n",
    "        split_train_data,\n",
    "        split_train_labels,\n",
    "        m_best, s_best, rho_best, gamma,\n",
    "        random_state=42,\n",
    "        is_dp=False\n",
    "    )\n",
    "\n",
    "    best_y_test = y_test\n",
    "    best_predictions = best_flynn_model.predict(X_test_scaled)\n",
    "\n",
    "    noise_levels = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "    n_bootstrap = 100\n",
    "\n",
    "    noise_results, p_values_acc, p_values_f1 = evaluate_flynn_robustness(\n",
    "        best_flynn_model, X_test_scaled, y_test, noise_levels, n_bootstrap)\n",
    "\n",
    "    logging.info(\"===== NOISE ROBUSTNESS RESULTS WITH P-VALUES =====\")\n",
    "    logging.info(f\"{'Noise Level':<12} {'Accuracy':<12} {'F1 Score':<12} {'p-val (Acc)':<12} {'p-val (F1)':<12}\")\n",
    "\n",
    "    logging.info(f\"{noise_results[0][0]:<12} {noise_results[0][1]:.7f}      {noise_results[0][2]:.7f}      -            -\")\n",
    "\n",
    "    for i in range(1, len(noise_results)):\n",
    "        logging.info(f\"{noise_results[i][0]:<12} {noise_results[i][1]:.7f}      {noise_results[i][2]:.7f}      {p_values_acc[i-1]:.2e}     {p_values_f1[i-1]:.2e}\")\n",
    "\n",
    "    logging.info(\"FlyNN process completed.\")\n",
    "\n",
    "    return best_params, best_accuracies, best_precisions, best_recalls, best_f1s, best_times, best_client_times, best_comm_costs\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_flynn()"
   ],
   "id": "7cd45668f9fb30c4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T08:12:20.290082Z",
     "start_time": "2025-05-18T08:12:20.286619Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "577d3f077db9e133",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ceb2441f0a82809b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
